{
	"tapdata": {
		"Parameters":"参数",
		"Notifications": "消息",
		"Error Records": "错误记录",
		"Cluster": "集群",
		"Execution Mode": "运行模式",
		"JDBC Connection String": "JDBC 连接字符串",
		"The number of retries upon a SQL Error.  To allow for the possibility of handling transient errors (ex: connection issues, deadlock, etc.), provide a positive value here.  After the specified number of retries is reached, the stage will fail upon the next error and pipeline error handling behavior will take over.":"SQL错误时重试的次数。 为了能够处理瞬时错误（例如：连接问题，死锁等），请在此处提供一个正值。 达到指定的重试次数后，阶段将在下一个错误发生时失败，并且管道错误处理行为将接管。",
		"Produce new records for each element of a list or map field": "为列表或地图字段的每个元素生成新记录",
	"Language": "语言",
	"Field Pivoter": "现场枢轴",
	"Collection Cloning":"Collection复制",
	"Check this if replicating mongo collection": "如果复制mongo collection,请勾选该选项",
	"Listens for requests on an HTTP endpoint": "在HTTP端点上侦听请求",
	"HTTP Server": "HTTP服务器",
	"Flattens nested structures.": "压扁嵌套结构。",
	"Field Flattener": "现场拼合",
	"Decodes a Base64 encoded Byte Array field": "解码Base64编码的字节数组字段",
	"Base64 Field Decoder": "Base64字段解码器",
	"Encodes a Byte Array field into a Base64 encoded Byte Array": "将字节数组字段编码为Base64编码的字节数组",
	"Base64 Field Encoder": "Base64现场编码器",
	"Uses an HTTP client to write data.": "使用HTTP客户端来写入数据。",
	"HTTP Client": "HTTP客户端",
	"Sends records to a Named Pipe": "将记录发送到命名管道",
	"Named Pipe": "命名管道",
	"Separates unique and duplicate records based on field comparison": "根据字段比较分离唯一和重复的记录",
	"Record Deduplicator": "记录重复数据删除者",
	"Order MAP or LIST_MAP fields into LIST_MAP or LIST.": "将MAP或LIST_MAP字段排序为LIST_MAP或LIST。",
	"Field Order": "现场秩序",
	"Rename fields": "重命名字段",
	"Field Renamer": "场更名",
	"Aggregates data that arrives within a window of time": "汇总在一段时间内到达的数据",
	"Aggregator": "聚合",
	"Uses an OPC UA Client to read data from an OPC UA Server.": "使用OPC UA客户端从OPC UA服务器读取数据。",
	"OPC UA Client": "OPC UA客户端",
	"Converts the data type of a field(s)": "转换字段的数据类型",
	"Field Type Converter": "字段类型转换器",
	"Write to Another Pipeline": "写入另一个管道",
	"Sends records to the pipeline configured error records handling": "将记录发送到管道配置的错误记录处理",
	"To Error": "错误",
	"Performs calculations on a field-by-field basis": "按字段执行计算",
	"Expression Evaluator": "表达评估者",
	"Writes pipeline statistics to SCH directly without any aggregation": "将流水线统计信息直接写入SCH，不需要任何聚合",
	"Write to SCH directly": "直接写入SCH",
	"Allows you to delay any records passing through it by a given number of milliseconds": "允许您将通过它的任何记录延迟给定的毫秒数",
	"Delay": "延迟",
	"Listens for TCP messages on one or more ports": "在一个或多个端口上侦听TCP消息",
	"TCP Server": "TCP服务器",
	"Uses an HTTP client to make arbitrary requests.": "使用HTTP客户端进行任意请求。",
	"Writes records to a local File System as SDC records": "将记录作为SDC记录写入本地文件系统",
	"Write to File": "写入文件",
	"Tails a file. It handles rolling files within the same directory": "尾巴文件。它处理相同目录中的滚动文件",
	"File Tail": "文件尾巴",
	"Uses a CoAP client to write data": "使用CoAP客户端来写入数据",
	"CoAP Client": "CoAP客户端",
	"Replaces null values with a constant and replaces values with NULL": "用一个常量替换空值并用NULL替换值",
	"Value Replacer": "价值替代品",
	"Parses a string field with XML data": "用XML数据解析字符串字段",
	"XML Parser": "XML解析器",
	"Reads files from a directory": "从目录中读取文件",
	"Directory": "目录",
	"Uses a WebSocket client to read from a resource URL": "使用WebSocket客户端从资源URL中读取数据",
	"WebSocket Client": "WebSocket客户端",
	"Listens for UDP messages on one or more ports": "在一个或多个端口上侦听UDP消息",
	"UDP Source": "UDP源",
	"Discards records and events": "丢弃记录和事件",
	"Discard": "丢弃",
	"Writes pipeline Statistic records to another pipeline over SDC RPC": "通过SDC RPC将管道统计信息记录写入另一个管道",
	"Write to SDC RPC": "写入SDC RPC",
	"Uses an MQTT client to subscribe to a topic on the MQTT Broker": "使用MQTT客户端订阅MQTT代理上的主题",
	"MQTT Subscriber": "MQTT订户",
	"Replaces field values.": "替换字段值。",
	"Field Replacer": "现场替代品",
	"Discards Pipeline Statistic Records": "丢弃管道统计记录",
	"Uses an algorithm to hash field values": "使用算法来散列字段值",
	"Field Hasher": "领域哈斯",
	"Merge fields of like types": "合并类似类型的字段",
	"Field Merger": "现场合并",
	"Uses an HTTP client to read records from an URL.": "使用HTTP客户端从URL读取记录。",
	"Serializes a field to a string field in JSON format": "将字段序列化为JSON格式的字符串字段",
	"JSON Generator": "JSON生成器",
	"Removes fields from a record": "从记录中删除字段",
	"Field Remover": "现场卸妆",
	"Masks field values": "掩码字段值",
	"Field Masker": "场Masker",
	"Listens for requests on a WebSocket endpoint": "在WebSocket端点上侦听请求",
	"WebSocket Server": "WebSocket服务器",
	"Discards records": "丢弃记录",
	"Trash": "垃圾",
	"IP address geolocation using a Maxmind GeoIP2 database file": "使用Maxmind GeoIP2数据库文件的IP地址地理定位",
	"Geo IP": "地理IP",
	"Receives records via SDC RPC from a Data Collector pipeline that uses an SDC RPC destination. It buffers records in memory/disk. In case of failure/stop records may be lost.": "通过SDC RPC从使用SDC RPC目标的Data Collector管道接收记录。它缓存内存/磁盘中的记录。如果失败/停止记录可能会丢失。",
	"Dev SDC RPC with Buffering": "具有缓冲的开发SDC RPC",
	"Listens for requests on a CoAP endpoint": "侦听CoAP端点上的请求",
	"CoAP Server": "CoAP服务器",
	"Sends emails upon receipt of specific events": "收到特定事件后发送电子邮件",
	"Email Executor": "电子邮件执行者",
	"Splits a string field based on a separator character": "根据分隔符分割字符串字段",
	"Field Splitter": "场分离器",
	"Uses an SFTP/FTP client to read records from an URL.": "使用SFTP / FTP客户端从URL读取记录。",
	"SFTP FTP Client": "SFTP FTP客户端",
	"Parses a string field with JSON data": "使用JSON数据解析字符串字段",
	"JSON Parser": "JSON解析器",
	"Writes error records to MQTT broker": "将错误记录写入MQTT代理",
	"Write to MQTT": "写入MQTT",
	"Flatten XML data into fields of a record": "将XML数据平铺到记录的字段中",
	"XML Flattener": "XML拼合器",
	"Passes records to streams based on conditions": "根据条件将记录传递给流",
	"Stream Selector": "流选择器",
	"Performs key-value lookups in static table.": "在静态表中执行键值查找。",
	"Static Lookup": "静态查找",
	"Writes to the local file system": "写入本地文件系统",
	"Local FS": "本地FS",
	"Receives records via SDC RPC from a Data Collector pipeline that uses an SDC RPC destination": "通过SDC RPC从使用SDC RPC目标的Data Collector管道接收记录",
	"SDC RPC": "SDC RPC",
	"Sends records via SDC RPC to a Data Collector pipeline that uses an SDC RPC origin": "通过SDC RPC将记录发送到使用SDC RPC原点的Data Collector管道",
	"Uses a WebSocket client to write data": "使用WebSocket客户端写入数据",
	"Uses an MQTT Client to publish data to a topic on the MQTT Broker": "使用MQTT客户端将数据发布到MQTT代理上的主题",
	"MQTT Publisher": "MQTT发布商",
	"Parses a field with data": "用数据分析字段",
	"Data Parser": "数据解析器",
	"Zips two lists together into one.": "将两个列表合并为一个。",
	"Field Zip": "现场邮编",
	"Processes records using JavaScript": "使用JavaScript处理记录",
	"JavaScript Evaluator": "JavaScript评估器",
	"Parses a string field which contains a Log line": "分析包含Log行的字符串字段",
	"Log Parser": "日志解析器",
	"Generate schema based on incoming records.": "根据传入记录生成模式。",
	"Schema Generator": "架构生成器",
	"Executor allowing execution of a custom shell script.": "执行器允许执行一个自定义的shell脚本。",
	"Shell": "贝壳",
	"Forces pipeline to transition to Finished after receiving an event.": "强制管道在收到事件后转换为Finished。",
	"Pipeline Finisher Executor": "管道修整器执行器",
	"Listens for UDP messages on one or more port(s) and queues incoming packets on an intermediate queue, from which multiple worker threads can process them": "在一个或多个端口上侦听UDP消息，并在中间队列上对传入数据包进行排队，多个工作线程可以从中间队列处理它们",
	"UDP Multithreaded Source": "UDP多线程源",
	"Reads data from a Windows event log": "从Windows事件日志中读取数据",
	"Windows Event Log": "Windows事件日志",
	"Sensor Reader. For development only.": "感应器。仅用于开发。",
	"Sensor Reader": "感应器",
	"Generates error records and silently discards records as specified.": "生成错误记录并按指定的方式静静地丢弃记录。",
	"Dev Random Error": "开发随机错误",
	"Generates records with the specified field names based on the selected data type. For development only.": "根据所选数据类型生成具有指定字段名称的记录。仅用于开发。",
	"Dev Data Generator": "开发数据生成器",
	"It echoes root field from records as a body of an event. For development purpose only.": "它将记录的根域作为事件的主体来回应。仅用于开发目的。",
	"To Event": "事件",
	"Generates records with the specified field names, using Long data. For development only.": "使用长数据生成具有指定字段名称的记录。仅用于开发。",
	"Dev Random Record Source": "开发随机记录源",
	"It creates 2 records from each original record": "它从每个原始记录创建2条记录",
	"Dev Record Creator": "开发记录创建者",
	"It echoes every record it receives without changing, other than stage header information": "除了舞台标题信息之外，它会回应它收到的每个记录而不会改变",
	"Dev Identity": "开发标识",
	"Add Raw data to the source.": "将原始数据添加到源。",
	"Dev Raw Data Source": "开发原始数据源",
	"Aggregates Pipeline Metrics": "汇总管道指标",
	"Writes data to Http destination": "将数据写入Http目标",
	"Http Destination": "Http目的地",
	"Writes data to MongoDB": "将数据写入MongoDB",
	"MongoDB": "MongoDB",
	"Reads OpLog records from MongoDB": "从MongoDB读取OpLog记录",
	"MongoDB Oplog": "MongoDB Oplog",
	"Reads records from a MongoDB collection": "从MongoDB collection中读取记录",
	"Lookup values via JDBC to enrich records.": "通过JDBC查找值来丰富记录。",
	"JDBC Lookup": "JDBC查找",
	"Origin that an read change events from an SQL Server Database": "读取来自SQL Server数据库的更改事件的来源",
	"SQL Server Change Tracking Client": "SQL Server更改跟踪客户端",
	"Reads data from a JDBC source using a query.": "使用查询从JDBC源读取数据。",
	"JDBC Query Consumer": "JDBC查询使用者",
	"Write records to JDBC and enrich records with generated columns": "将记录写入JDBC并使用生成的列丰富记录",
	"JDBC Tee": "JDBC Tee",
	"Executes queries against JDBC compliant database": "执行对JDBC兼容数据库的查询",
	"JDBC Query": "JDBC查询",
	"Origin that an read change events from an MS SQL Server Database": "从MS SQL Server数据库读取更改事件的起源",
	"SQL Server CDC Client": "SQL Server CDC客户端",
	"Insert, update, delete data to a JDBC destination.": "插入，更新和删除数据到JDBC目标。",
	"JDBC Producer": "JDBC生产者",
	"Origin that an read change events from an Oracle Database": "读取源自Oracle数据库的更改事件",
	"Oracle CDC Client": "Oracle CDC客户端",
	"Reads data from a JDBC source using table names.": "使用表名从JDBC源读取数据。",
	"JDBC Multitable Consumer": "JDBC多用户",
	"Create/alter tables in Postgres to match record structure": "在Postgres中创建/更改表以匹配记录结构",
	"Postgres Metadata": "Postgres元数据",
	"NULL value": "NULL值",
	"Field Type Integer": "字段类型整数",
	"Field Type Boolean": "字段类型布尔值",
	"Field Type Byte": "字段类型字节",
	"Field Type Byte Array": "字段类型字节数组",
	"Field Type Char": "字段类型字符",
	"Field Type Date": "字段类型日期",
	"Field Type Time": "字段类型时间",
	"Field Type Date Time": "字段类型日期时间",
	"Field Type Decimal": "字段类型十进制",
	"Field Type Double": "字段类型双",
	"Field Type Float": "字段类型浮点数",
	"Field Type List": "字段类型列表",
	"Field Type Map": "字段类型映射",
	"Field Type List-Map": "字段类型列表 - 映射",
	"Field Type Long": "字段类型长",
	"Field Type Short": "字段类型短",
	"Field Type String": "字段类型字符串",
	"Kilobytes": "千字节",
	"Megabytes": "兆字节",
	"Gigabytes": "千兆字节",
	"Returns base64 encoded version of the string argument.": "返回字符串参数的base64编码版本。",
	"Returns a decoded string from a base64 encoded string argument and charset name.": "从base64编码的字符串参数和字符集名称返回已解码的字符串。",
	"Returns a decoded byte array from a base64 encoded string argument.": "从base64编码的字符串参数返回解码的字节数组。",
	"Returns parent path to given file or directory. Returns path without separator (e.g. result will not end with slash).": "返回给定文件或目录的父路径。返回没有分隔符的路径（例如，结果不会以斜杠结尾）。",
	"Returns file extension from given path (e.g. 'txt' from /path/file.txt).": "从给定路径返回文件扩展名（例如，从/path/file.txt中的'txt'）。",
	"Returns path without the extension (e.g. '/path/file' from /path/file.txt).": "返回没有扩展名的路径（例如，/ path / file.txt中的'/ path / file'）。",
	"Returns element (directory or file name) from given index. On path /path/to/file.txt, 'path' have index 0, 'to' have index 1 and file.txt have index 2.": "从给定索引返回元素（目录或文件名）。在路径/path/to/file.txt中，'path'的索引为0，'to'的索引为1，file.txt的索引为2。",
	"Returns just file name from given path.": "从给定路径返回文件名。",
	"JVM Maximum Heap size, in MB": "JVM最大堆大小（以MB为单位）",
	"Returns absolute value of the argument.": "返回参数的绝对值。",
	"Return minimal value of given two numbers.": "返回给定两个数字的最小值。",
	"Returns the greater of two numbers.": "返回两个数字中较大的一个。",
	"Returns the smallest (closest to negative infinity) double value that is greater than or equal to the argument and is equal to a mathematical integer.": "返回大于或等于参数的最小（最接近负无穷大）double值，并且等于一个数学整数。",
	"Returns the largest (closest to positive infinity) double value that is less than or equal to the argument and is equal to a mathematical integer.": "返回小于或等于参数的最大（最接近正无穷大）double值，并且等于一个数学整数。",
	"Returns the closest int or long to the argument, with ties rounding up.": "返回距离参数最近的整数或长整数，并且四舍五入。",
	"Return hostname where SDC runs": "返回SDC运行的主机名",
	"Returns the auth token of this data collector": "返回此数据收集器的身份验证令牌",
	"Retrieves the value of the config property from sdc runtime configuration": "从sdc运行时配置中检索config属性的值",
	"Loads the contents of a file under the Data Collector resources directory. If restricted is set to 'true', the file must be readable only by its owner.": "加载Data Collector资源目录下的文件内容。如果restricted被设置为'true'，则该文件必须只能由其所有者读取。",
	"Loads the contents of a file including any leading and trailing whitespace under the Data Collector resources directory. If restricted is set to 'true', the file must be readable only by its owner.": "加载包含Data Collector资源目录下的任何前导和尾随空白的文件的内容。如果restricted被设置为'true'，则该文件必须只能由其所有者读取。",
	"Returns the number of CPU cores as reported by Java": "返回Java报告的CPU核心数量",
	"Converts all of the characters in the argument string to uppercase": "将参数字符串中的所有字符转换为大写",
	"Returns the string length of the string argument.": "返回字符串参数的字符串长度。",
	"Returns a randomly generated UUID.": "返回一个随机生成的UUID。",
	"Returns true if the string is null or empty": "如果字符串为空或空，则返回true",
	"Captures the string that matches the argument regular expression and the group": "捕获与参数正则表达式和组匹配的字符串",
	"Returns URL encoded variant of the string.": "返回字符串的URL编码变体。",
	"Returns decoded string from URL encoded variant.": "从URL编码变体返回解码后的字符串。",
	"Returns a string safe to embed in an XML 1.0 or 1.1 document.": "返回一个可安全嵌入到XML 1.0或1.1文档中的字符串。",
	"Returns a string safe to embed in an XML 1.1 document.": "返回一个可安全嵌入XML 1.1文档的字符串。",
	"Returns an unescaped string from a string with XML special characters escaped.": "从转义了XML特殊字符的字符串中返回未转义的字符串。",
	"Returns each element of a LIST field joined on the specified character sequence.": "返回在指定字符序列上连接的LIST字段的每个元素。",
	"Returns each element of a LIST field joined on the specified character sequence skipping null values.": "返回在指定字符序列上加入的LIST字段的每个元素，跳过空值。",
	"Splits key value pairs into a map": "将键值对分解成一张地图",
	"Returns the index within the string of the first occurrence of the specified substring.": "返回指定子字符串第一次出现的字符串中的索引。",
	"Indicates whether the argument string starts with the specified prefix": "指示参数字符串是否以指定的前缀开头",
	"Indicates whether argument string ends with the specified suffix": "指示参数字符串是否以指定的后缀结尾",
	"Returns a new string that is a substring of this string. The substring begins at the specified beginIndex and extends to the character at index endIndex-1. Thus the length of the substring is endIndex-beginIndex": "返回一个新字符串，该字符串是此字符串的一个子字符串。子字符串从指定的beginIndex开始，并扩展到索引endIndex-1处的字符。因此子字符串的长度是endIndex-beginIndex",
	"Returns a new string that is a concatenation of the two argument strings.": "返回一个新的字符串，它是两个参数字符串的串联。",
	"Returns a new string resulting from replacing all occurrences of oldString in this string with newString": "返回由newString替换此字符串中出现的所有oldString而产生的新字符串",
	"Tells whether the argument string matches the argument regex.": "告诉参数字符串是否与参数regex匹配。",
	"Indicates whether the argument string contains the specified substring.": "指示参数字符串是否包含指定的子字符串。",
	"Replaces each substring of this string that matches the given regEx with the given replacement": "将此字符串中与给定regEx匹配的每个子字符串替换为给定的替换字符串",
	"Removes leading and trailing whitespaces": "删除前导和尾随空格",
	"Converts all of the characters in the argument string to lowercase": "将参数字符串中的所有字符转换为小写",
	"Truncates the argument string to the given index": "将参数字符串截断为给定的索引",
	"Returns an unescaped string from a string with Java special characters (e.g.  n will be converted to 0x0A).": "从具有Java特殊字符的字符串中返回未转义的字符串（例如， n将转换为0x0A）。",
	"Returns user who started this pipeline.": "返回启动此管道的用户。",
	"Returns the title of the pipeline if applicable. Returns \"UNDEFINED\" otherwise": "返回管道的标题（如果适用）。否则返回“UNDEFINED”",
	"Returns the name of the pipeline": "返回管道的名称",
	"Returns the version of the pipeline if applicable. Returns \"UNDEFINED\" otherwise": "返回管道的版本（如果适用）。否则返回“UNDEFINED”",
	"Returns the id of the pipeline if applicable. Returns \"UNDEFINED\" otherwise": "如果适用，则返回管道的标识。否则返回“UNDEFINED”",
	"Returns a record header attribute": "返回记录标题属性",
	"Returns the error message for the record in context": "返回上下文中记录的错误消息",
	"Returns the error code for the record in context": "返回上下文中记录的错误代码",
	"Returns the error stack trace for the record in context": "返回上下文中记录的错误堆栈跟踪",
	"Returns the error stage for the record in context": "返回上下文中记录的错误阶段",
	"Returns the value of the field represented by path 'fieldPath' for the record in context or the default value if the field is not present or if the field is null": "返回上下文记录中由路径'fieldPath'表示的字段的值，或者如果该字段不存在或者该字段为null，则返回默认值",
	"Returns the value of the attribute represented by 'attributeName' for the record in context or the default value if the attribute is not present or if the attribute is null": "如果该属性不存在或属性为null，则返回上下文中记录的'attributeName'所表示的属性值或默认值",
	"Returns the value of the attribute named 'attributeName' of the field specified by 'fieldPath'": "返回由'fieldPath'指定的字段名为'attributeName'的属性的值",
	"Returns the value of the attribute named 'attributeName' of the field specified by 'fieldPath' or the 'defaultValue' if not found": "如果未找到，则返回由'fieldPath'或'defaultValue'指定的字段名为'attributeName'的属性的值",
	"Returns the id of the record in context": "返回上下文中记录的ID",
	"Returns the stage path for the record in context": "返回上下文中记录的阶段路径",
	"Returns the error stage label for the record in context": "在上下文中返回记录的错误阶段标签",
	"Returns the error data collector id for the record in context": "返回上下文中记录的错误数据收集器标识",
	"Returns the error pipeline name for the record in context": "在上下文中返回记录的错误管道名称",
	"Returns the error time for the record in context": "返回上下文中记录的错误时间",
	"Returns type of the event for event records and null for non-event records.": "返回事件记录事件的类型，非事件记录返回null。",
	"Returns version of the event for event records and null for non-event records.": "返回事件记录的事件版本，非事件记录返回null。",
	"Returns creation time of the event for event records and null for non-event records.": "返回事件记录事件的创建时间，非事件记录返回null。",
	"Returns the value of the specified header name": "返回指定标题名称的值",
	"Converts Columns to a Map": "将列转换为地图",
	"Returns the index of the specific header name": "返回特定标题名称的索引",
	"Returns if a header is more than once": "如果标题不止一次，则返回",
	"Returns if there are duplicate headers": "返回是否有重复标题",
	"Returns the value of the field represented by path 'fieldPath' for the record in context": "为上下文中的记录返回路径'fieldPath'表示的字段的值",
	"Returns the type of the field represented by path 'fieldPath' for the record in context": "返回上下文中记录的路径'fieldPath'表示的字段的类型",
	"Checks if the field represented by path 'fieldPath' exists in the record": "检查记录中是否存在由路径“fieldPath”表示的字段",
	"Retrieves the credential of the specified key from the specified store with store specific options. The user must belong the specified group.": "使用特定于商店的选项从指定商店检索指定密钥的凭证。用户必须属于指定的组。",
	"Retrieves the credential of the specified key frmn the specified store. The user must belong the specified group": "从指定商店中检索指定密钥的凭证。用户必须属于指定的组",
	"Retrieves the value of the specified path in Vault.": "检索Vault中指定路径的值。",
	"Retrieves the value of the specified path in Vault and waits for delay milliseconds.Primarily for AWS since generated credentials can take 5-10 seconds before they are ready for use.": "检索Vault中指定路径的值并等待延迟毫秒。主要针对AWS，因为生成的凭据可能需要5-10秒才能使用。",
	"Creates a Datetime object set to the current time.": "创建一个设置为当前时间的Datetime对象。",
	"Set date portion of datetime expression to January 1, 1970": "将日期时间表达式的日期部分设置为1970年1月1日",
	"Set time portion of datetime expression to 00:00:00": "将日期时间表达式的时间部分设置为00:00:00",
	"Convert epoch in milliseconds to DateTime": "以毫秒为单位将时期转换为DateTime",
	"Convert DateTime to epoch in milliseconds": "将DateTime转换为以毫秒为单位的纪元",
	"Return timezone's offset in milliseconds. Pass empty timezone to get offset for 'current' timezone.": "以毫秒为单位返回时区的偏移量。传递空时区以获得'当前'时区的偏移量。",
	"Return timezone's offset in milliseconds of given date and timezone. Pass empty timezone to get offset for 'current' timezone.": "以给定日期和时区的毫秒为单位返回时区的偏移量。传递空时区以获得'当前'时区的偏移量。",
	"Format a date into a string, based on an output format specification": "根据输出格式规范将日期格式化为字符串",
	"Format a date into a long, based on an output format specification": "根据输出格式规范将日期格式化为长整型",
	"Format a String date into a date.": "将字符串日期格式化为日期。",
	"Format a Date into a string date, adjusting for time zone.": "将日期格式化为字符串日期，并根据时区进行调整。",
	"Change String Date / append Timezone to a datetime object": "更改字符串日期/将时区附加到日期时间对象",
	"Creates an empty map": "创建一个空的地图",
	"Returns true if a list is empty": "如果列表为空，则返回true",
	"Returns true if a map is empty": "如果地图为空，则返回true",
	"Creates an empty list": "创建一个空列表",
	"Returns the length of a list": "返回列表的长度",
	"Returns the size of a map": "返回地图的大小",
	"Unique ID for the SDC": "SDC的唯一ID",
	"Returns the specified attribute value of the field in context": "返回上下文中字段的指定属性值",
	"Returns the value of the field in context": "返回上下文中字段的值",
	"Returns the type of the field in context": "返回上下文中字段的类型",
	"Returns the value of the path to the field in context, with respect to its containing record": "返回上下文中相对于其包含记录的字段路径的值",
	"generates uuid": "生成uuid",
	"Returns the positioned Offset Column for the current table": "返回当前表格的定位偏移列",
	"Information about the Drift Data rule that triggered the alert": "有关触发警报的“漂移数据”规则的信息",
	"Triggers an alert if the type of the specified field changes": "如果指定字段的类型更改，则触发警报",
	"Triggers an alert if the number of entries in the specified LIST, MAP or LIST_MAP field changes": "如果指定的LIST，MAP或LIST_MAP字段中的条目数发生更改，则触发警报",
	"Triggers an alert if the keys of the entries in the specified MAP or LIST_MAP field changes": "如果指定的MAP或LIST_MAP字段中的条目的键发生更改，则触发警报",
	"Triggers an alert if the order of the entries in the specified LIST_MAP field changes": "如果指定的LIST_MAP字段中的条目顺序发生更改，则触发警报",
	"Execution Mode": "执行模式",
	"Delivery Guarantee": "数据送达保证",
	"Stage that should handle pipeline start event.": "应该处理管道启动事件的阶段。",
	"Start Event": "开始活动",
	"Stage that should handle pipeline stop event.": "应该处理管道停止事件的阶段。",
	"Stop Event": "停止事件",
	"Retry Pipeline on Error": "错误时重试管道",
	"Max no of retries. To retry indefinitely, use -1. The wait time between retries starts at 15 seconds and doubles until reaching 5 minutes.": "最大不重试次数。要无限期重试，请使用-1。两次重试之间的等待时间从15秒开始，双倍增加至5分钟。",
	"Retry Attempts": "重试尝试",
	"Maximum amount of memory the pipeline can use. Configure in relationship to the SDC Java heap size. The default is 85% of heap and a value of 0 disables the limit.": "管道可以使用的最大内存量。配置与SDC Java堆大小的关系。缺省值是堆的85％，值为0会禁用此限制。",
	"Max Pipeline Memory (MB)": "最大流水线内存（MB）",
	"Behavior when the pipeline exceeds the memory limit. Tip: Configure an alert to indicate when the memory use approaches the limit.": "管道超过内存限制时的行为。提示：配置警报以指示内存使用量接近限制的时间。",
	"On Memory Exceeded": "超过内存",
	"Notifies via email when pipeline gets to the specified states": "当管道达到指定状态时通过电子邮件通知",
	"Notify on Pipeline State Changes": "通知管道状态更改",
	"Email Addresses": "电子邮件地址",
	"Email IDs": "电子邮件ID",
	"Parameters": "参数",
	"Error Records": "错误日志",
	"Determines which variation of the record is sent to error.": "确定记录的哪个变化发送到错误。",
	"Error Record Policy": "错误记录策略",
	"Statistics Aggregator": "统计聚合器",
	"Number of workers. 0 to start as many workers as Kafka partitions for topic.": "工人数量。 0开始尽可能多的工人作为主题的卡夫卡分区。",
	"Worker Count": "工人计数",
	"Worker Memory (MB)": "工作者记忆（MB）",
	"Add properties as needed. Changes to default settings are not recommended.": "根据需要添加属性。不建议更改默认设置。",
	"Worker Java Options": "工作者Java选项",
	"Sets additional environment variables for the cluster launcher": "为集群启动器设置其他环境变量",
	"Launcher ENV": "启动器ENV",
	"URL for service which launches Mesos framework": "启动Mesos框架的服务的URL",
	"Mesos Dispatcher URL": "Mesos调度程序URL",
	"An SDC resource directory or symbolic link with HDFS/S3 configuration files core-site.xml and hdfs-site.xml": "HDC / S3配置文件core-site.xml和hdfs-site.xml的SDC资源目录或符号链接",
	"Checkpoint Configuration Directory": "检查点配置目录",
	"Maximum number of records per second that should be accepted into the pipeline. Rate is not limited if this is not set, or is set to 0": "应该接受到管道中的最大每秒记录数。如果没有设置或者设置为0，速率不受限制",
	"Rate Limit (records / sec)": "费率限制（记录/秒）",
	"Maximum number of runners that should be created for this pipeline. Use 0 to not impose limit.": "应该为此管道创建的最大跑步者数量。使用0不限制。",
	"Max runners": "最大跑步者",
	"When selected and the pipeline execution fails with unrecoverable exception, SDC will attempt to createpartial snapshot with records that have not been processed yet.": "如果选中并且管道执行失败并显示不可恢复的异常，则SDC将尝试使用尚未处理的记录创建部分快照。",
	"Create Failure Snapshot": "创建失败快照",
	"Webhooks": "网络挂接",
	"Additional Spark Configuration to pass to the spark-submit script, the parameters will be passed as --conf <key>=<value>": "额外的Spark Configuration传递给spark-submit脚本，参数将以--conf <key> = <value>的形式传递",
	"Extra Spark Configuration": "额外的火花配置",
	"Path to the field that will be exploded into multiple records (supported types are LIST and LIST_MAP).": "将被分解为多个记录的字段的路径（支持的类型是LIST和LIST_MAP）。",
	"Field To Pivot": "字段转轴",
	"Copy all fields (including the original list) to each resulting record. If this is not set, then the pivoted value is set as the root field of the record.": "将所有字段（包括原始列表）复制到每个结果记录。如果没有设置，那么pivoted值被设置为记录的根字段。",
	"Copy All Fields": "复制所有字段",
	"Path in the new record where the pivoted list items are written to. Each record will contain oneitem from the original list at this path. If this is not specified, the path of the original list is used. If there is data at this field path, it will be overwritten.": "写入透视列表项目的新记录中的路径。每条记录将包含此路径原始列表中的一个项目。如果未指定，则使用原始列表的路径。如果此字段路径中有数据，则会被覆盖。",
	"Pivoted Items Path": "Pivoted Items Path",
	"Specifies whether or not to save the original field name of the pivoted field.": "指定是否保存旋转字段的原始字段名称。",
	"Save Original Field Name": "保存原始字段名称",
	"Path in the new record to store the name of the field that was pivoted.": "新记录中的路径，用于存储已旋转字段的名称。",
	"Original Field Name Path": "原始字段名称路径",
	"Action for data that does not contain the specified fields": "对不包含指定字段的数据采取的行动",
	"Field Does Not Exist": "字段不存在",
	"Action to take with records sent to error": "采取记录发送给错误的操作",
	"On Record Error": "记录错误",
	"Records without any of these fields are sent to error": "没有包含这些必需字段的记录将被发送到错误记录",
	"Required Fields": "必填字段",
	"Records that don't satisfy all the preconditions are sent to error": "不满足所有先决条件的记录将被发送至错误",
	"Preconditions": "前提条件",
	"Enable transport layer security for this stage.": "为此阶段启用传输层安全性。",
	"Use TLS": "使用TLS",
	"The path to the keystore file.  Absolute path, or relative to the Data Collector resources directory.": "密钥库文件的路径。绝对路径，或相对于Data Collector资源目录。",
	"Keystore File": "密钥库文件",
	"The type of certificate/key scheme to use for the key tore.": "用于密钥的证书/密钥方案的类型。",
	"Keystore Type": "密钥库类型",
	"The password to the keystore file, if applicable.  Using a password is highly recommended for security reasons.": "密钥库文件的密码（如果适用）。出于安全原因，强烈建议使用密码。",
	"Keystore Password": "密钥库密码",
	"The key manager algorithm to use with the keystore.": "与密钥库一起使用的密钥管理器算法。",
	"Keystore Key Algorithm": "密钥库密钥算法",
	"Use only modern TLS protocols (TLSv1.2).  This is highly recommended for security reasons, but can be overridden if special circumstances require it.": "只使用现代TLS协议（TLSv1.2）。出于安全考虑，强烈建议这样做，但如果特殊情况需要，可以重写。",
	"Use Default Protocols": "使用默认协议",
	"The transport protocols to enable for connections (ex: TLSv1.2, TLSv1.1, etc.).": "用于连接的传输协议（例如：TLSv1.2，TLSv1.1等）。",
	"Transport Protocols": "运输协议",
	"Use only modern cipher suites.  This is highly recommended for security reasons, but can be overridden if special circumstances require it.": "只使用现代密码套件。出于安全考虑，强烈建议这样做，但如果特殊情况需要，可以重写。",
	"Use Default Cipher Suites": "使用默认密码套件",
	"The cipher suites for connections (ex: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, etc.).": "用于连接的密码套件（例如：TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384等）。",
	"Cipher Suites": "密码套件",
	"HTTP endpoint to listen for data.": "HTTP端点来监听数据。",
	"HTTP Listening Port": "HTTP侦听端口",
	"Maximum number of concurrent requests allowed by the origin.": "源的最大并发请求数。",
	"Max Concurrent Requests": "最大并发请求",
	"Only HTTP requests presenting this token will be accepted.": "只有呈现该令牌的HTTP请求才会被接受。",
	"Application ID": "应用程序ID",
	"Use when the application ID is included in a query parameter in the URL instead of in the request header - http://localhost:8000?sdcApplicationId=<Application ID>": "当应用程序标识包含在URL中的查询参数中而不是请求标头中时使用 - http：// localhost：8000？sdcApplicationId = <Application ID>",
	"Application ID in URL": "应用程序ID在URL中",
	"Max Request Size (MB)": "最大请求大小（MB）",
	"HTTP payload data format": "HTTP有效载荷数据格式",
	"Data Format": "数据格式",
	"Compression formats gzip, bzip2, xz, lzma, Pack200, DEFLATE and Z are supported. Archive formats 7z, ar, arj, cpio, dump, tar and zip are supported.": "压缩格式支持gzip，bzip2，xz，lzma，Pack200，DEFLATE和Z.存档格式支持7z，ar，arj，cpio，dump，tar和zip。",
	"Compression Format": "压缩格式",
	"A glob pattern that defines the pattern of the file names within the compressed directory.": "一个glob模式，用于定义压缩目录中文件名的模式。",
	"File Name Pattern within Compressed Directory": "压缩目录中的文件名称模式",
	"Charset": "字符集",
	"Use only if required as it impacts reading performance": "仅在需要时使用，因为它会影响阅读表现",
	"Ignore Control Characters": "忽略控制字符",
	"Longer lines are truncated": "更长的行被截断",
	"Max Line Length": "最大线路长度",
	"Use custom delimiters to create records": "使用自定义分隔符来创建记录",
	"Use Custom Delimiter": "使用自定义分隔符",
	"One or more characters. Leading and trailing spaces are stripped.": "一个或多个字符。前后空格被剥离。",
	"Custom Delimiter": "自定义分隔符",
	"Include custom delimiters in the data": "在数据中包含自定义分隔符",
	"Include Custom Delimiter": "包括自定义分隔符",
	"JSON Content": "JSON内容",
	"Larger objects are not processed": "不处理较大的对象",
	"Max Object Length (chars)": "最大对象长度（字符）",
	"Delimiter Format Type": "分隔符格式类型",
	"Header Line": "标题行",
	"When false, rows with more columns than the header are sent to error.": "如果为false，则具有比标题更多列的行将发送至错误。",
	"Allow Extra Columns": "允许额外的列",
	"Each extra column is labeled with this prefix followed by an integer": "每个额外的列标有这个前缀后跟一个整数",
	"Extra Column Prefix": "额外的列前缀",
	"Max Record Length (chars)": "最大记录长度（字符）",
	"Delimiter Character": "分隔符字符",
	"Escape Character": "逃脱字符",
	"Quote Character": "引用字符",
	"Enable comments": "启用评论",
	"Comment marker": "评论标记",
	"Ignore empty lines": "忽略空行",
	"Root Field Type": "根域类型",
	"Number of lines to skip before reading": "阅读前要跳过的行数",
	"Lines to Skip": "要跳过的行",
	"When checked, configured string constant will be converted into NULL field.": "选中时，配置的字符串常量将被转换为NULL字段。",
	"Parse NULLs": "解析NULL",
	"String constant that should be converted to a NULL rather then passed as it is.": "应该转换为NULL的字符串常量，然后按原样传递。",
	"NULL constant": "NULL常量",
	"XML element that acts as a record delimiter.  This may be an element name, with elements having that local name directly under the root as records.  Or, it can be a simplified XPath expression (see docs), with elements matching the XPath expression as records.  Leaving it blank will treat the whole XML document as one record.": "充当记录分隔符的XML元素。这可能是一个元素名称，元素具有作为记录直接在根下的本地名称。或者，它可以是简化的XPath表达式（请参阅文档），其元素与XPath表达式匹配为记录。如果将其留空，则会将整个XML文档视为一条记录。",
	"Delimiter Element": "分隔符元素",
	"Include XPath expressions that indicate the path to the input node that was parsed to create each field.  If enabled, each field will receive an attribute named \"xpath\" whose value is the XPath expression.  Any namespace prefixes will be mapped to the full URI via new xmlns:* attributes in the record header.": "包含XPath表达式，这些表达式指示已解析以创建每个字段的输入节点的路径。如果启用，每个字段将接收名为“xpath”的属性，其值是XPath表达式。任何名称空间前缀都将通过记录标题中的新xmlns：*属性映射到完整的URI。",
	"Include Field XPaths": "包含字段XPath",
	"Namespace context to use if the delimiter is an XPath expression.  This should map namespace prefixes to URIs.  Any namespace prefix that is used in the record separator expression must be defined here.": "如果分隔符是XPath表达式，则使用名称空间上下文。这应该将名称空间前缀映射到URI。在这里必须定义记录分隔符表达式中使用的任何名称空间前缀。",
	"Namespaces": "命名空间",
	"Generate field attributes in output record for XML namespace declarations, and XML attributes.  Without this option, they will continue to be output as individual fields (separate from the element value).": "在输出记录中为XML名称空间声明和XML属性生成字段属性。没有这个选项，它们将继续作为单独的字段输出（与元素值分开）。",
	"Output Field Attributes": "输出字段属性",
	"Larger records are not processed": "较大的记录不被处理",
	"Log Format": "日志格式",
	"Indicates if the original line of log should be retained in the record": "指示是否应将原始日志行保留在记录中",
	"Retain Original Line": "保留原始行",
	"Custom Log Format": "自定义日志格式",
	"The regular expression which is used to parse the log line.": "用于解析日志行的正则表达式。",
	"Regular Expression": "正则表达式",
	"Map groups in the regular expression to field paths": "将正则表达式中的组映射到字段路径",
	"Field Path To RegEx Group Mapping": "RegEx组映射的字段路径",
	"Define your own grok patterns which will be used to parse the logs": "定义你自己的grok模式，用来解析日志",
	"Grok Pattern Definition": "Grok模式定义",
	"The grok pattern which is used to parse the log line": "用于解析日志行的grok模式",
	"Grok Pattern": "Grok模式",
	"On Parse Error": "解析错误",
	"Any line that does not match the expected pattern will be treated as a Stack trace as long as it is part of the same message. The stack trace will be trimmed to the specified number of lines.": "只要它是相同消息的一部分，任何与预期模式不匹配的行都将被视为堆栈跟踪。堆栈轨迹将被修剪为指定的行数。",
	"Trim Stack Trace to Length": "修剪堆栈跟踪到长度",
	"Use Custom Log Format": "使用自定义日志格式",
	"Specify your own custom log4j format.": "指定您自己的自定义log4j格式。",
	"Custom Log4J Format": "自定义Log4J格式",
	"Overrides the schema included in the data (if any). Optionally use the runtime:loadResource function to use a schema stored in a file.": "覆盖数据中包含的模式（如果有的话）。可以选择使用runtime：loadResource函数来使用存储在文件中的模式。",
	"Avro Schema": "Avro Schema",
	"List of Confluent Schema Registry URLs": "Confluent架构注册表URL列表",
	"Schema Registry URLs": "架构注册网址",
	"Whether to look up the Avro Schema by ID or fetch the latest schema for a Subject.": "无论是通过ID查找Avro Schema还是获取Subject的最新模式。",
	"Lookup Schema By": "查找模式依据",
	"Schema Subject": "架构主题",
	"Schema ID": "模式ID",
	"Protobuf Descriptor File (.desc) path relative to SDC resources directory": "Protobuf描述符文件（.desc）相对于SDC资源目录的路径",
	"Protobuf Descriptor File": "Protobuf描述符文件",
	"Fully Qualified Message Type name. Use format <packageName>.<messageTypeName>": "完全合格的消息类型名称。使用格式<packageName>。<messageTypeName>",
	"Message Type": "消息类型",
	"Should be checked when the input data is prepended with the message size. When unchecked only a single message can be present in the source file/Kafka message, etc.": "输入数据前面加上消息大小时应该检查。未选中时，只有一条消息可以出现在源文件/ Kafka消息中，等等",
	"Delimited Messages": "分隔邮件",
	"Max Data Size (bytes)": "最大数据大小（字节）",
	"Datagram Packet Format": "数据包格式",
	"User-specified TypesDB file. Overrides the included version.": "用户指定的TypesDB文件。覆盖包含的版本。",
	"TypesDB File Path": "TypesDB文件路径",
	"Converts high resolution time format interval and timestamp to unix time in (ms).": "将高分辨率时间格式间隔和时间戳转换为以（ms）为单位的unix时间。",
	"Convert Hi-Res Time & Interval": "转换高分辨率时间和间隔",
	"Excludes the interval field from output records.": "排除输出记录中的间隔字段。",
	"Exclude Interval": "排除间隔",
	"Auth File": "身份验证文件",
	"Determines the data that is included in the record generated from a flow. Only applies to NetFlow 9.": "确定从流中生成的记录中包含的数据。仅适用于NetFlow 9。",
	"Record Generation Mode": "记录生成模式",
	"Controls the maximum number of templates to cache from all sources. Additional templates received when this limit is reached cause the eviction of existing templates, least recently used first. Leave as -1 for unlimited. Only applies to NetFlow 9.": "控制从所有来源缓存的模板的最大数量。达到此限制时收到的其他模板会导致现有模板的驱逐，最近最早使用的模板。为-1表示无限制。仅适用于NetFlow 9。",
	"Max Templates in Cache": "缓存中的最大模板",
	"Controls the maximum length of time flow templates are cached, after last being used to parse a data flow. Leave as -1 for unlimited (never expires). Only applies to NetFlow 9.": "控制最后一次用于分析数据流之后，流模板被缓存的最大时间长度。为-1表示无限制（永不过期）。仅适用于NetFlow 9。",
	"Template Cache Timeout (ms)": "模板高速缓存超时（ms）",
	"Size of the Buffer used to copy the file.": "用于复制文件的缓冲区的大小。",
	"Buffer Size (bytes)": "缓冲区大小（字节）",
	"Rate / sec to manipulate bandwidth requirements for File Transfer. Use <= 0 to opt out. Default unit is B/sec": "Rate / sec来操纵文件传输的带宽需求。使用<= 0退出。默认单位是B /秒",
	"Rate per second": "每秒钟的速率",
	"Select what should be flattened in the record": "选择应在记录中展开的内容",
	"Flatten": "弄平",
	"List of fields to be flattened": "字段的列表被拼合",
	"Fields": "字段",
	"When set, each filed will be flatten in place.": "设置时，每个字段将被放平。",
	"Flatten in Place": "展平",
	"Field (must be MAP or MAP_LIST) into which the fields should be flattened.": "字段（必须是MAP或MAP_LIST），字段应该放在该字段中。",
	"Target Field": "目标字段",
	"Action that should be performed when he target field already have field with given name.": "目标字段时应执行的操作已具有给定名称的字段。",
	"Collision Field Action": "碰撞现场行动",
	"When set, flattened filed will be removed after successful flattening.": "设置后，平坦的场地将在平坦成功后移除。",
	"Remove Flattened Field": "删除拼合的字段",
	"Separator that is used when created merged field name from nested structures.": "从嵌套结构创建合并字段名称时使用的分隔符。",
	"Name separator": "名称分隔符",
	"Base64 encoded Byte Array field that is to be decoded": "将被解码的Base64编码的字节数组字段",
	"Field to Decode": "字段解码",
	"Target field to which decoded Byte Array is to be written to": "解码字节数组要写入的目标字段",
	"Byte Array field that is to be encoded": "要编码的字节数组字段",
	"Field to Encode": "字段编码",
	"Target field to which encoded Byte Array is to be written to": "编码字节数组要写入的目标字段",
	"Encode the field so that it can be safely sent in a URL": "对字段进行编码，以便可以通过URL安全地发送该字段",
	"URL Safe": "网址安全",
	"Delimiter Format": "分隔符格式",
	"Replaces new lines characters with configured string constant": "用配置的字符串常量替换新行字符",
	"Replace New Line Characters": "替换新行字符",
	"String that will be used to substitute new line characters. Using empty string will remove the new line characters.": "将用于替换换行符的字符串。使用空字符串将删除新的行字符。",
	"New Line Character Replacement": "新行字符替换",
	"String field that will be written to the destination": "将被写入目标的字符串字段",
	"Text Field Path": "文本字段路径",
	"Value to insert in output between records, defaults to newline": "插入记录之间输出的值，默认为换行",
	"Record Separator": "记录分隔符",
	"On Missing Field": "论失踪领域",
	"Specifies whether a record separator should be inserted in output even after an empty value (no text in field)": "指定是否应该在输出中插入记录分隔符（即使在空值之后）（字段中没有文本）",
	"Insert Record Separator If No Text": "如果没有文字，插入记录分隔符",
	"Where to load the Avro Schema from.": "从哪里加载Avro Schema。",
	"Avro Schema Location": "Avro Schema位置",
	"Overrides the schema included in the data (if any). Optionally use the runtime:loadResource function to use a schema stored in a file": "覆盖数据中包含的模式（如果有的话）。可以选择使用runtime：loadResource函数来使用存储在文件中的模式",
	"Register the Avro schema in the Confluent Schema Registry": "在Confluent模式注册表中注册Avro模式",
	"Register Schema": "注册架构",
	"If this and Schema Registry URLs are non-empty, will register the supplied schema.": "如果这个和模式注册表URL不是空的，将注册提供的模式。",
	"Includes the Avro schema in the output": "在输出中包含Avro模式",
	"Include Schema": "包含架构",
	"Avro Compression Codec": "Avro压缩编解码器",
	"Field to write data to Kafka": "将数据写入卡夫卡的字段",
	"Binary Field Path": "二进制字段路径",
	"File Name Expression": "文件名称表达",
	"The action to perform when the file already exists.": "文件已存在时要执行的操作。",
	"File Exists": "文件已存在",
	"Includes checksum information in whole file transfer events.": "包括整个文件传输事件中的校验和信息。",
	"Include Checksum in Events": "在事件中包含校验和",
	"The checksum algorithm for calculating checksum for the file.": "用于计算文件校验和的校验和算法。",
	"Checksum Algorithm": "校验和算法",
	"Format XML with human readable indentation (requires more bytes on output).": "使用人类可读缩进格式化XML（输出中需要更多字节）。",
	"Pretty Format": "漂亮的格式",
	"Validate that resulting record corresponds to given schema(s).": "验证结果记录对应于给定的模式。",
	"Validate Schema": "验证架构",
	"XML schema that should be used to validate serialized record.": "应该用来验证序列化记录的XML模式。",
	"XML Schema": "XML架构",
	"Data Format of the response. Response will be parsed before being placed in the record.": "响应的数据格式。响应在被放入记录之前将被解析。",
	"The HTTP resource URL": "HTTP资源URL",
	"Resource URL": "资源网址",
	"Headers to include in the request": "包含在请求中的头部",
	"Headers": "头",
	"HTTP method to send": "HTTP方法发送",
	"HTTP Method": "HTTP方法",
	"Expression used to determine the HTTP method to use": "用于确定要使用的HTTP方法的表达式",
	"HTTP Method Expression": "HTTP方法表达式",
	"Request Transfer Encoding": "请求传输编码",
	"HTTP Compression": "HTTP压缩",
	"HTTP connection timeout in milliseconds. 0 means no timeout.": "HTTP连接超时（以毫秒为单位）。 0意味着没有超时。",
	"Connect Timeout": "连接超时",
	"HTTP read timeout in milliseconds. 0 means no timeout.": "HTTP读取超时（以毫秒为单位）。 0意味着没有超时。",
	"Read Timeout": "读取超时",
	"Maximum number of requests to make in parallel.": "并行请求的最大数量。",
	"Maximum Parallel Requests": "最大并行请求",
	"Authentication Type": "认证类型",
	"Use OAuth 2 to get access tokens": "使用OAuth 2获取访问令牌",
	"Use OAuth 2": "使用OAuth 2",
	"OAuth Consumer Key": "OAuth使用者密钥",
	"Consumer Key": "消费者钥匙",
	"OAuth Consumer Secret": "OAuth消费者秘密",
	"Consumer Secret": "消费者秘密",
	"OAuth Consumer Token": "OAuth消费者令牌",
	"Token": "代币",
	"OAuth Token Secret": "OAuth令牌秘密",
	"Token Secret": "令牌秘密",
	"Credentials Grant Type": "凭证授权类型",
	"Token URL": "令牌网址",
	"Client ID": "客户端ID",
	"Client Secret": "客户秘密",
	"Username": "用户名",
	"Password": "密码",
	"The algorithm to use for signing the JWT": "用于签署JWT的算法",
	"JWT Signing Algorithm": "智威汤逊签名算法",
	"Base64 encoded key for signing the JWT": "用于签署JWT的Base64编码密钥",
	"JWT Signing Key (Base64-encoded)": "智威汤逊签名密钥（Base64编码）",
	"Claims to be used with JWT token request, represented as JSON": "与JWT令牌请求一起使用的声明，表示为JSON",
	"JWT Claims": "智威汤逊索赔",
	"Additional key-value pairs to be sent to the token URL while requesting for a token": "在请求令牌时发送到令牌URL的其他键 - 值对",
	"Additional Key-Value pairs in token request body": "标记请求正文中的附加键值对",
	"Use Proxy": "使用代理服务器",
	"Proxy URI": "代理URI",
	"The path to the truststore file.  Absolute path, or relative to the Data Collector resources directory.": "信任库文件的路径。绝对路径，或相对于Data Collector资源目录。",
	"Truststore File": "信任库文件",
	"The type of certificate/key scheme to use for the truststore.": "用于信任库的证书/密钥方案的类型。",
	"Truststore Type": "信任库类型",
	"The password to the truststore file, if applicable.  Using a password is highly recommended for security reasons.": "信任库文件的密码（如果适用）。出于安全原因，强烈建议使用密码。",
	"Truststore Password": "信任库密码",
	"The key manager algorithm to use with the truststore.": "与信任库一起使用的密钥管理器算法。",
	"Truststore Trust Algorithm": "信任库信任算法",
	"Enable logging of HTTP request and response data.": "启用HTTP请求和响应数据的记录。",
	"Enable Request Logging": "启用请求记录",
	"The log level to use when logging messages (from java.util.logging package).": "记录消息时使用的日志级别（来自java.util.logging包）。",
	"Log Level": "日志级别",
	"The verbosity to use when logging messages (i.e. which type of request/response data).": "记录消息时使用的详细程度（即哪种类型的请求/响应数据）。",
	"Verbosity": "赘言",
	"The maximum entity size to log (in bytes). Entities larger than this value will not be logged.": "记录的最大实体大小（以字节为单位）。大于此值的实体将不会被记录。",
	"Max Entity Size": "最大实体大小",
	"Generates a single HTTP request with all records in the batch": "生成一个包含批处理中所有记录的HTTP请求",
	"One Request per Batch": "每批一个请求",
	"Maximum requests per second (0 for unlimited). Useful for rate-limited APIs.": "每秒最大请求数（0表示无限制）。对速率有限的API非常有用。",
	"Rate Limit (Requests/sec)": "费率限制（请求/秒）",
	"Maximum time to wait for each request completion.": "等待每个请求完成的最长时间。",
	"Maximum Request Time (sec)": "最大请求时间（秒）",
	"Full path of the Named Pipe": "命名管道的完整路径",
	"Format of data in the files": "文件中数据的格式",
	"Max Records to Compare": "最大记录比较",
	"Creates a window of time for comparison. Takes precedence over Max Records. Use 0 for no time window.": "创建一个时间窗口进行比较。优先于Max Records。没有时间窗口使用0。",
	"Time to Compare (secs)": "比较时间（秒）",
	"Compare": "比较",
	"Fields to Compare": "要比较的字段",
	"List of fields in the desired order": "以所需顺序列出字段",
	"Fields to Order": "订单的字段",
	"Type of the output parent field": "输出父字段的类型",
	"Output Type": "输出类型",
	"Action to perform if the record doesn't have all the fields specified in the order list": "如果记录没有在订单列表中指定的所有字段，则执行该操作",
	"Missing Fields": "缺少字段",
	"Default value to be inserted for missing fields": "要为缺少的字段插入默认值",
	"Default Value": "默认值",
	"Data type of the default value": "数据类型的默认值",
	"Default Type": "默认类型",
	"Action to perform if the record has additional fields that weren't specified in the order list": "要执行的操作，如果记录具有未在顺序列表中指定的其他字段",
	"Extra Fields": "额外的领域",
	"List of fields that should be discarded if they appear in the source record": "如果它们出现在源记录中应该被丢弃的字段的列表",
	"Discard Fields": "放弃字段",
	"Fields to rename, and target field names.": "要重命名的字段和目标字段名称。",
	"Fields to Rename": "要重命名的字段",
	"Response when records do not include the specified source fields.": "当记录不包括指定的源字段时作出响应。",
	"Source Field Does Not Exist": "源字段不存在",
	"Response when records include field names that match the specified target fields.": "记录中包含与指定的目标字段匹配的字段名称时作出响应。",
	"Target Field Already Exists": "目标字段已存在",
	"Response when source fields match multiple source field regular expressions.": "源字段与多个源字段正则表达式匹配时的响应。",
	"Multiple Source Field Matches": "多个源字段匹配",
	"Window Type": "窗口类型",
	"Time Window": "时间窗口",
	"Time zone to use for time windows": "用于时间窗口的时区",
	"Time Zone": "时区",
	"Number of Time Windows to Remember": "要记住的时间窗数量",
	"Aggregations": "聚合",
	"Produce one event record for all aggregations computed by this processor": "为此处理器计算的所有聚合生成一个事件记录",
	"All Aggregators Event": "所有聚合器事件",
	"Produce one record for each aggregation computed by this processor": "为该处理器计算的每个聚合生成一条记录",
	"Per Aggregator Events": "每个聚合器事件",
	"When this option is chosen the aggregation data is written in a String field as a JSON string in the event record": "选择此选项时，聚合数据将作为JSON字符串写入事件记录中的字符串字段中",
	"Produce Event Record with Text Field": "使用文本字段生成事件记录",
	"Specify the OPC UA resource URL": "指定OPC UA资源URL",
	"Specify the OPC UA client application name": "指定OPC UA客户端应用程序名称",
	"Application Name": "应用名称",
	"Application URI": "应用程序URI",
	"OPC UA request timeout in milliseconds.": "OPC UA请求超时（以毫秒为单位）。",
	"Request Timeout": "请求超时",
	"OPC UA session timeout in milliseconds.": "OPC UA会话超时（以毫秒为单位）。",
	"Session Timeout": "会话超时",
	"Processing Mode": "处理模式",
	"Polling Interval (ms)": "轮询间隔（毫秒）",
	"Security Policy": "安全策略",
	"An alias is specified when you add an entity to the keystore using the -genseckey command to generate a secret key, -genkeypair command to generate a key pair (public and private key).": "使用-genseckey命令生成密钥时，使用-genkeypair命令生成密钥对（公钥和私钥）时，将实体添加到密钥库时会指定别名。",
	"Client Private Key Alias": "客户机私钥别名",
	"NodeId Fetch Mode": "NodeId获取模式",
	"Fields to generate of the indicated Node Id": "生成指定节点ID的字段",
	"File path to the NodeId file. Or an expression that points to the correct runtime resource file.": "文件路径到NodeId文件。或者指向正确的运行时资源文件的表达式。",
	"NodeId File Path": "NodeId文件路径",
	"The identifier for a root node in the address space of an OPC UA server": "OPC UA服务器地址空间中根节点的标识符",
	"Root Node Identifier": "根节点标识符",
	"The format and data type of the identifier": "标识符的格式和数据类型",
	"Root Node Identifier Type": "根节点标识符类型",
	"The index an OPC UA server uses for a namespace URI": "OPC UA服务器用于命名空间URI的索引",
	"Root Node Namespace Index": "根节点名称空间索引",
	"Refresh interval for updating Node IDs by browsing root Node ID.": "通过浏览根节点ID刷新节点ID的刷新间隔。",
	"NodeId Refresh Interval (sec)": "NodeId刷新间隔（秒）",
	"The maximum size of a single chunk. Must be greater than or equal to 8192": "单个块的最大大小。必须大于或等于8192",
	"Max Chunk Size": "最大块大小",
	"The maximum number of chunks that a message can break down into.": "消息可以分解为的最大数量的块。",
	"Max Chunk Count": "最大块数",
	"The maximum size of a message after all chunks have been assembled. Default value Max Chunk Size *  Max Chunk Count = (2mb)": "所有组块之后的最大消息大小。默认值最大块大小*最大块大小=（2mb）",
	"Max Message Size": "最大邮件大小",
	"Max Array Length": "最大数组长度",
	"Max String Length": "最大字符串长度",
	"Select type of conversion that will be performed.": "选择将要执行的转换类型。",
	"Conversion Method": "转换方法",
	"Configures field by names that should be converted": "按应该转换的名称配置字段",
	"Configure types that should be converted. All fields of given type in a record will be converted.": "配置应该转换的类型。记录中给定类型的所有字段都将被转换。",
	"Connection information for the destination pipeline. Use the format <host>:<port>.": "目标管道的连接信息。使用格式<host>：<port>。",
	"SDC RPC Connection": "SDC RPC连接",
	"User-defined ID. Must match the SDC RPC ID used in the SDC RPC origin of the destination pipeline.": "用户定义的ID。必须与目标管道的SDC RPC源中使用的SDC RPC ID匹配。",
	"SDC RPC ID": "SDC RPC ID",
	"Disables server certificate hostname verification": "禁用服务器证书主机名验证",
	"Verify Host In Server Certificate": "验证服务器证书中的主机",
	"Retries per Batch": "每批次重试",
	"If set to non-zero, each retry will be spaced exponentially. For value 10, first retry will be done after 10 milliseconds, second retry after additional 100 milliseconds, third retry after additional second, ... The maximum wait time is 5 minutes.": "如果设置为非零，每个重试将按指数间隔。对于值10，第一次重试将在10毫秒后完成，第二次重试在额外的100毫秒后重试，第三次在额外的秒后重试，...最长等待时间为5分钟。",
	"Back off period": "退后期",
	"Connection Timeout (ms)": "连接超时（毫秒）",
	"Read Timeout (ms)": "读取超时（毫秒）",
	"Use Compression": "使用压缩",
	"Field Expressions": "现场表达",
	"Header Attribute Expressions": "标题属性表达式",
	"Field Attribute Expressions": "字段属性表达式",
	"Milliseconds to wait before sending records to next stage": "在将记录发送到下一个阶段之前等待毫秒",
	"Delay Between Batches": "批次间延迟",
	"When checked verifies the checksum of the stream during read.": "选中时验证流的校验和。",
	"Verify Checksum": "验证校验和",
	"Port to listen on": "端口监听",
	"Port": "港口",
	"Enable epoll transports.  Multithreaded performance will be significantly higher with this option. Only available on 64-bit Linux systems": "启用epoll传输。使用此选项，多线程性能将显着提高。仅适用于64位Linux系统",
	"Enable Native Transports (Epoll)": "启用本地传输（Epoll）",
	"Number of receiver threads for each port. It should be based on the CPU cores expected to be dedicated to the pipeline": "每个端口的接收器线程数。它应该基于预计专用于管道的CPU内核",
	"Number of Receiver Threads": "接收器线程数",
	"The mode the TCP server operates in, based on the expected input data format": "TCP服务器根据预期的输入数据格式运行的模式",
	"TCP Mode": "TCP模式",
	"The TCP syslog message transfer mode to be used, as defined in RFC 6587.  Method change is not allowed (i.e. must be consistent between all clients and sessions).": "RFC 6587中定义的要使用的TCP系统日志消息传输模式。不允许使用方法更改（即必须在所有客户端和会话之间保持一致）。",
	"Syslog Message Transfer Framing Mode": "系统日志消息传输成帧模式",
	"When using non-transparent-framing, this is the separator character that will appear between separate syslog messages.  Specify using Java Unicode syntax (\" uxxxx\").  Defaults to line feed (000A).": "当使用非透明成帧时，这是将在单独的系统日志消息之间出现的分隔符。指定使用Java Unicode语法（“ uxxxx”）。默认为换行（000A）。",
	"Non-transparent-framing Separator": "非透明框架分隔符",
	"The character encoding that Syslog messages will have": "Syslog消息将具有的字符编码",
	"When using delimited records mode, this is the separator character that will appear between separate records.  Specify using Java Unicode syntax (\" uxxxx\").  Defaults to line feed (000A).": "使用分隔记录模式时，这是将出现在单独记录之间的分隔符。指定使用Java Unicode语法（“ uxxxx”）。默认为换行（000A）。",
	"Record separator": "记录分隔符",
	"The character encoding that the character data with length prefix messages use. Note that the length digits themselves, plus space, must be in a single byte encoding.": "带长度前缀消息的字符数据使用的字符编码。请注意，长度数字本身加上空格必须采用单字节编码。",
	"Max Batch Size (messages)": "最大批量（消息）",
	"Max time to wait for data before sending a batch": "发送批次之前等待数据的最长时间",
	"Batch Wait Time (ms)": "批量等待时间（毫秒）",
	"Max message size in bytes": "最大邮件大小以字节为单位",
	"Max Message Size (bytes)": "最大邮件大小（字节）",
	"The character set to be used when sending ack messages back to client": "向客户端发送确认消息时要使用的字符集",
	"Time zone to use for ack message evaluation (if time or time now ELs are used)": "用于确认消息评估的时区（如果现在使用EL的时间或时间）",
	"Ack Time Zone": "确认时区",
	"Acknowledgement message to be sent back to the client upon each successfully processed record.": "确认消息在每个成功处理的记录上发送回客户端。",
	"Record Processed Ack Message": "记录已处理的确认消息",
	"Acknowledgement message to be sent back to the client upon each successfully completed batch. The record in the EL context is the last record processed in the batch.": "确认消息在每个成功完成的批次中发送回客户端。 EL环境中的记录是批次中处理的最后一条记录。",
	"Batch Completed Ack Message": "批量完成确认消息",
	"Field in which to place the result of the HTTP request": "放置HTTP请求结果的字段",
	"Output Field": "输出字段",
	"Header Output Location": "标题输出位置",
	"A prefix to add to record header attributes in the response": "在响应中添加记录标题属性的前缀",
	"Header Attribute Prefix": "标题属性前缀",
	"Field in which to place the HTTP response headers.": "放置HTTP响应标题的字段。",
	"Header Output Field": "标题输出字段",
	"Data that should be included as a part of the request": "应作为请求的一部分包含的数据",
	"Request Data": "请求数据",
	"Content-Type header to be sent with the request; used if that header is not already present": "内容类型头部与请求一起发送;如果该标题尚不存在，则使用该标记",
	"Default Request Content Type": "默认请求内容类型",
	"Time between requests (in ms, 0 for unlimited). Useful for rate-limited APIs.": "请求之间的时间（以毫秒为单位，0表示无限制）。对速率有限的API非常有用。",
	"Rate Limit (ms)": "速率限制（毫秒）",
	"Directory to write records": "目录写入记录",
	"File name prefix": "文件名称前缀",
	"Files Prefix": "文件前缀",
	"Max time to wait for error records before creating a new error file.": "在创建新错误文件之前最长等待错误记录的时间。",
	"Enter the time in seconds or use the default expression to enter the time limit in minutes. You can also use HOURS in the expression to enter the limit in hours.": "输入以秒为单位的时间或使用默认表达式以分钟为单位输入时间限制。您也可以在表达式中使用HOURS以小时为单位输入限制。",
	"File Wait Time (secs)": "文件等待时间（秒）",
	"Max file size to trigger the creation of a new file. Use 0 to opt out.": "最大文件大小触发创建新文件。使用0退出。",
	"Max File Size (MB)": "最大文件大小（MB）",
	"The data format in the files (IMPORTANT: if Log, Log4j files with stack traces are not handled)": "文件中的数据格式（重要说明：如果不处理带有堆栈跟踪的Log，Log4j文件）",
	"RegEx pattern to detect main lines for Text and Log files with multi-line elements. Use only if required as it impacts reading performance": "RegEx模式，用于检测具有多行元素的文本和日志文件的主线。仅在需要时使用，因为它会影响阅读表现",
	"Pattern for Multiline": "多线模式",
	"Max number of lines that will be sent in a single batch": "将在单个批次中发送的最大行数",
	"Maximum Batch Size": "最大批量大小",
	"Maximum amount of time to wait to fill a batch before sending it": " 在发送之前等待批量填充的最长时间",
	"Batch Wait Time (secs)": "批量等待时间（秒）",
	"File to Tail": "文件尾巴",
	"Enables reading from late-arriving directories. When enabled, the origin does not validate configured paths.": "从晚到目录启用阅读。启用后，原点不验证配置的路径。",
	"Allow Late Directories": "允许延迟目录",
	"Action to take after processing a file": "处理文件后采取的措施",
	"File Post Processing": "文件后期处理",
	"Directory to archive files after they have been processed": "目录在文件处理后对其进行归档",
	"Archive Directory": "档案目录",
	"The CoAP resource URL": "CoAP资源URL",
	"CoAP method to send": "CoAP方法发送",
	"CoAP Method": "CoAP方法",
	"Specify the type of requests.": "指定请求的类型。",
	"Request Type": "请求类型",
	"CoAP connection timeout in milliseconds. 0 means no timeout.": "CoAP连接超时（以毫秒为单位）。 0意味着没有超时。",
	"Conditionally Replaces existing values with null values": "有条件地用空值替换现有的值",
	"Fields to Null": "字段为空",
	"Replaces the null values in a field with a specified value.": "用指定值替换字段中的空值。",
	"Replace Null Values": "替换空值",
	"Conditionally replaces values with new specified value.": "有条件地用新的指定值替换值。",
	"Conditionally Replace Values": "有条件替换值",
	"String field that contains a XML document": "包含XML文档的字符串字段",
	"Field to Parse": "字段解析",
	"Name of the field to set the parsed XML data to": "将解析的XML数据设置为的字段的名称",
	"How to handle multiple values produced by the parser": "如何处理解析器产生的多个值",
	"Multiple Values Behavior": "多值行为",
	"Use a local directory": "使用本地目录",
	"Files Directory": "文件目录",
	"Number of parallel threads that read data": "读取数据的并行线程数",
	"Number of Threads": "线程数",
	"Select whether the File Name Pattern specified uses glob pattern syntax or regex syntax.": "选择指定的文件名称模式是否使用全局模式语法或正则表达式语法。",
	"File Name Pattern Mode": "文件名称模式模式",
	"A glob or regular expression that defines the pattern of the file names in the directory.": "定义目录中文件名称模式的glob或正则表达式。",
	"File Name Pattern": "文件名称模式",
	"Read files based on the last-modified timestamp or lexicographically ascending file names. When timestamp ordering is used, files with the same timestamp are ordered based on file names.": "根据上次修改的时间戳或按字典顺序升序的文件名读取文件。使用时间戳排序时，具有相同时间戳的文件将根据文件名进行排序。",
	"Read Order": "阅读订单",
	"Process files in subdirectories of Files Directory.  Only file names matching File Name Pattern will be processed.": "在文件目录的子目录中处理文件。只有与文件名称模式匹配的文件名将被处理。",
	"Process Subdirectories": "处理子目录",
	"Enables reading from a late-arriving directory. When enabled, the origin does not validate the configured path.": "启用从迟到目录中读取数据。启用后，原点不验证配置的路径。",
	"Allow Late Directory": "允许晚期目录",
	"Low level reader buffer limit to avoid out of Memory errors": "低级读取器缓冲区限制，以避免内存错误",
	"Buffer Limit (KB)": "缓冲区限制（KB）",
	"Max number of records per batch": "每批最多记录数",
	"Batch Size (recs)": "批量（recs）",
	"Max time to wait for new files before sending an empty batch": "在发送空批次之前等待新文件的最长时间",
	"Maximum number of files added to the processing queue at one time. This is a soft limit and can be temporarily exceeded.": "一次添加到处理队列的最大文件数量。这是一个软限制，可以暂时超出。",
	"Max Files Soft Limit": "最大文件软限制",
	"Max time period to spool the files": "假脱机文件的最大时间段",
	"Spooling Period (secs)": "后台打印时间（秒）",
	"When configured, the Data Collector does not process earlier (naturally ascending order) file names": "配置后，Data Collector不会处理更早（自然升序）的文件名",
	"First File to Process": "要处理的第一个文件",
	"Directory for files that could not be fully processed": "无法完全处理的文件的目录",
	"Error Directory": "错误目录",
	"How long archived files should be kept before deleting, a value of zero means forever": "在删除之前应该保留多久存档文件，值为零意味着永远",
	"Archive Retention Time (mins)": "存档保留时间（分钟）",
	"The WebSocket resource URL": "WebSocket资源URL",
	"WebSocket payload data format": "Max Record Size (MB)",
	"Use multiple receiver threads for each port. Only available on 64-bit Linux systems": "Max Fragments in Memory",
	"Use Native Transports (epoll)": "Max Disk Buffer (MB)",
	"Number of receiver threads for each port.  This controls the number of epoll threads bound to each port.": "WebSocket有效载荷数据格式",
	"The mode that controls how the raw packet data should be treated (character-based or binary). This selection determines what type of field will be created.": "为每个端口使用多个接收器线程。仅适用于64位Linux系统",
	"Raw Data Mode": "使用本机传输（epoll）",
	"The character set used to interpret character-based separated data.": "每个端口的接收器线程数。这将控制绑定到每个端口的epoll线程的数量。",
	"The output field path to place the separated data values into.": "控制如何处理原始数据包数据的模式（基于字符或二进制）。该选择决定了将创建什么类型的字段。",
	"Output field path": "原始数据模式",
	"How to handle multiple values produced by the parser after applying the separator.": "用于解释基于字符的分离数据的字符集。",
	"The bytes to use to separate data in the UDP packet.  If multiple values are found in a packet after applying this separator, then the Multiple Values Behavior setting comes into play..  Specify byte literals using using Java Unicode syntax (\" uxxxx\").  To capture the entire UDP packet (i.e. do not split using any delimiter), leave this blank.  Defaults to line feed (000A).": "用于放置分隔数据值的输出字段路径。",
	"Data Separator": "输出字段路径",
	"System pipeline will be started on the specified host and port. Use the format <host>:<port>.": "如何在应用分隔符后处理解析器生成的多个值。",
	"The id to be assigned to the system pipeline.": "用于分隔UDP数据包中数据的字节。如果在应用此分隔符后在数据包中找到多个值，则将启用多值行为设置。使用Java Unicode语法（“ uxxxx”）指定字节文字。要捕获整个UDP数据包（即不使用任何分隔符进行分割），请将其留空。默认为换行（000A）。",
	"Specify the MQTT Broker URL": "数据分隔符",
	"Broker URL": "系统管道将在指定的主机和端口上启动。使用格式<host>：<port>。",
	"Specify the MQTT Client ID. It must be unique across all clients connecting to the same server.": "要分配给系统管道的标识。",
	"Specify the quality of service to publish the message.": "指定MQTT代理URL",
	"Quality of Service": "经纪人网址",
	"Specify the persistence mechanism used to enable reliable messaging. For messages sent at least once (1) or exactly once (2) to be reliably delivered, messages must be stored on both the client and server until the delivery of the message is complete.": "指定MQTT客户端ID。它必须在连接到同一台服务器的所有客户端上都是唯一的。",
	"Client Persistence Mechanism": "指定发布消息的服务质量。",
	"Specify the directory for file-based Persistence Mechanism": "服务质量",
	"Client Persistence Data Directory": "指定用于启用可靠消息传递的持久性机制。对于至少一次（1）或完全一次（2）发送的消息以可靠的方式发送，消息必须存储在客户端和服务器上，直到消息发送完成。",
	"This value defines the maximum time interval between messages sent or received.": "客户端持久性机制",
	"Keep Alive Interval (secs)": "指定基于文件的持久性机制的目录",
	"Use Username and Password Authentication": "客户端持久性数据目录",
	"Use Credentials": "使用鉴权",
	"Specify the topics to subscribe to, which can include wildcards.": "保持活动间隔（秒）",
	"Topic Filter": "使用用户名和密码认证",
	"MQTT payload data format": "使用凭证",
	"Replacement rules": "指定要订阅的主题，其中可以包含通配符。",
	"Hash Entire Record": "主题过滤器",
	"Include the record header for hashing": "MQTT有效载荷数据格式",
	"Include Record Header": "替换规则",
	"Separate fields with null before hashing": "散列整个记录",
	"Use Field Separator": "包含散列的记录标题",
	"Hash Type": "包含记录标题",
	"String field to store the hashed value. Creates the field if it does not exist.": "散列前用空值分隔字段",
	"Header attribute to store the hashed value. Creates the attribute if it does not exist.": "使用字段分隔符",
	"Header Attribute": "散列类型",
	"Replaces data in the specified fields with the hashed values": "存储哈希值的字符串字段。如果该字段不存在，则创建该字段。",
	"Hash in Place": "标题属性存储哈希值。如果该属性不存在，则创建该属性。",
	"Hashes the specified fields and writes to a target field or header attribute. Multiple fields are hashed together.": "标题属性",
	"Hash to Target": "用散列值替换指定字段中的数据",
	"Action for data that does not contain the specified fields, the field value is null or if the field type is Map or List": "哈希到位",
	"On Field Issue": "散列指定的字段并写入目标字段或标题属性。多个字段一起散列。",
	"Fields to merge, and fields to merge into.": "哈希到目标",
	"Fields to merge": "对不包含指定字段的数据的操作，字段值为空或者字段类型为Map或List",
	"Action for data that does not contain the specified source field": "现场问题",
	"Whether or not to overwrite fields if a target field already exists": "要合并的字段以及合并到的字段。",
	"Overwrite Fields": "要合并的字段",
	"Max Batch Size (records)": "对不包含指定源字段的数据采取的行动",
	"Max time to wait for data before sending a partial or empty batch": "如果目标字段已存在，是否覆盖字段",
	"Specify the HTTP resource URL": "覆盖字段",
	"Time zone to use for request body evaluation (if time or time now ELs are used)": "最大批量（记录）",
	"Body Time Zone": "在发送部分或空批次之前等待数据的最长时间",
	"Data that should be included as the body of the request": "指定HTTP资源URL",
	"Request Body": "用于请求身体评估的时区（如果现在使用EL的时间或时间）",
	"Mode": "身体时区",
	"List of actions to take for specific response statuses.": "应作为请求正文包含的数据",
	"Per-Status Actions": "请求正文",
	"Action to take when the request times out (server does not respond within read timeout parameter)": "模式",
	"Action for timeout": "针对特定响应状态采取的操作列表。",
	"Base backoff interval in milliseconds.  Defaults to 1000 (1 second).": "按状态操作",
	"Base Backoff Interval (ms)": "请求超时时执行的操作（服务器在读取超时参数内未响应）",
	"The maximum number of times to retry the request before failing the stage.  A negative value will be treated as unlimited (i.e. infinite retries).": "超时行动",
	"Max Retries": "以毫秒为单位的基础退避时间间隔。默认为1000（1秒）。",
	"Pagination Mode": "基本退避时间间隔（毫秒）",
	"Field path in the response containing a URL to the next page.": "在失败阶段之前重试请求的最大次数。负值将被视为无限（即无限重试）。",
	"Next Page Link Field": "最大重试次数",
	"Expression that evaluates to true when there are no more pages to process.": "分页模式",
	"Stop Condition": "包含到下一页的URL的响应中的字段路径。",
	"Value of $ { startAt } variable the first time the pipeline is run or Reset Origin is invoked": "下一页链接字段",
	"Initial Page/Offset": "当没有更多页面要处理时，表达式的计算结果为true。",
	"Field path to parse as Records. The field type must be a list.": "停止条件",
	"Result Field Path": "第一次运行管道或重置原点时，$ {startAt}变量的值",
	"Includes all fields in the output record, rather than only fields from Result Field Path": "初始页面/偏移量",
	"Keep All Fields": "字段路径解析为记录。字段类型必须是列表。",
	"Time to wait between requests when paging (rate limit). E.g. 2000 would be 30 requests per minute": "结果字段路径",
	"Wait Time Between Pages (ms)": "包括输出记录中的所有字段，而不仅仅是结果字段路径中的字段",
	"Map or List field to serialize to JSON": "保持所有字段",
	"Field to Serialize": "分页时请求之间等待的时间（速率限制）。例如。 2000年将是每分钟30个请求",
	"Name of the field in which to place the serialized JSON string": "页面间等待时间（毫秒）",
	"Action": "映射或列表字段序列化为JSON",
	"WebSocket endpoint to listen for data.": "要序列化的字段",
	"WebSocket Listening Port": "放置序列化JSON字符串的字段的名称",
	"Only WebSocket requests presenting this token will be accepted.": "行动",
	"Use when the application ID is included in a query parameter in the URL instead of in the request header - ws://localhost:8000?sdcApplicationId=<Application ID>": "WebSocket端点来监听数据。",
	"Idle Timeout (ms)": "WebSocket侦听端口",
	"MaxMind GeoIP2 database file paths and database types.": "只有提供此令牌的WebSocket请求才会被接受。",
	"GeoIP2 Databases": "当应用程序ID包含在URL中的查询参数中而不是请求头中时使用 - ws：// localhost：8000？sdcApplicationId = <Application ID>",
	"Mappings of database fields to record fields.": "空闲超时（ms）",
	"Database Field Mappings": "MaxMind GeoIP2数据库文件路径和数据库类型。",
	"Action to perform on record if IP address is missing from database": "GeoIP2数据库",
	"Missing Address Action": "数据库字段的映射来记录字段。",
	"Port number to listen for data. Must match one of the port numbers used by the SDC RPC destination of the origin pipeline.": "数据库字段映射",
	"SDC RPC Listening Port": "如果数据库中缺少IP地址，则执行记录操作",
	"User-defined ID. Must match the SDC RPC ID used by the SDC RPC destination of the origin pipeline.": "缺少地址操作",
	"Maximum amount of time to wait for a batch before sending an empty one": "监听数据的端口号。必须匹配一个",
	"Wait Time for Empty Batches (millisecs)": "等待空批次的时间（毫秒）",
	"CoAP endpoint to listen for data.": "CoAP端点来监听数据。",
	"CoAP Listening Port": "CoAP侦听端口",
	"CoAP Resource Name": "CoAP资源名称",
	"Resource Name": "资源名称",
	"Additional network configuration properties. Values here override default values.": "其他网络配置属性。这里的值覆盖默认值。",
	"Network Configuration": "网络配置",
	"Configure Email Messages": "配置电子邮件",
	"Email Configuration": "电子邮件配置",
	"Field to Split": "场分裂",
	"Regular expression to use for splitting the field. If trying to split on a RegEx meta character \".$|()[{^?*+\", the character must be escaped with Separator": "用于分割字段的正则表达式。如果试图分割RegEx元字符“。$ |（）[{^？* + ”，则该字符必须用分隔器",
	"New fields to pass split data. The last field includes any remaining unsplit data.": "新字段传递拆分数据。最后一个字段包含任何未分离的数据。",
	"New Split Fields": "新的拆分字段",
	"Action for data that has fewer splits than configured field paths": "对配置字段路径的拆分数量少的操作",
	"Not Enough Splits": "没有足够的分割",
	"Action for data that more splits than configured field paths": "针对比配置的字段路径分裂的数据采取的行动",
	"Too Many Splits": "太多分裂",
	"List field used to store any remaining splits": "用于存储任何剩余分割的列表字段",
	"Field for Remaining Splits": "剩余分割字段",
	"Action for the original field being split": "对原始字段进行拆分的操作",
	"Original Field": "原始字段",
	"Specify the SFTP/FTP URL": "指定SFTP / FTP URL",
	"The authentication method to use to login to remote server": "用于登录到远程服务器的身份验证方法",
	"Authentication": "认证",
	"Username to use to login to the remote server": "用于登录到远程服务器的用户名",
	"Password to use to login to the remote server. If private key is specified, that is used.": "用于登录到远程服务器的密码。如果指定了私钥，则使用该私钥。",
	"Private Key File to use to login to the remote server.": "私钥文件用于登录到远程服务器。",
	"Private Key File": "私钥文件",
	"Passphrase to use to open the private key file.": "用于打开私钥文件的密码。",
	"Private Key Passphrase": "私钥密码",
	"If checked, the path is resolved relative to the logged in user's home directory, if a username is entered in the Credentials tab or in the URL.": "如果选中，则路径将相对于登录用户的主目录进行解析，如果用户名在“凭据”选项卡或URL中输入的话。",
	"Path Relative to User Home Directory": "相对于用户主目录的路径",
	"If enabled, this client will only connect to the host if the host is in the known hosts file.": "如果启用，则该客户端将只在主机位于已知主机文件中时连接到主机。",
	"Strict Host Checking": "严格的主机检查",
	"Full path to the file that lists the host keys of all known hosts.This must be specified if the strict host checking is enabled.": "列出所有已知主机的主机密钥的文件的完整路径。如果严格主机检查已启用，则必须指定此路径。",
	"Known Hosts file": "已知的主机文件",
	"On error, should the file be archive to a local directory": "出错时，应该将文件归档到本地目录",
	"Archive on error": "归档出错",
	"Directory to archive files, if an irrecoverable error is encountered": "目录归档文件，如果遇到不可恢复的错误",
	"Process files in subdirectories of the specified path": "处理指定路径的子目录中的文件",
	"A glob that defines the pattern of the file names in the directory. ('*' selects all files)Files are processed in chronological order.": "定义目录中文件名称模式的glob。 （'*'选择所有文件）文件按时间顺序处理。",
	"When configured, the Data Collector does not process earlier file names": "配置时，Data Collector不处理较早的文件名",
	"String field that contains a JSON object": "包含JSON对象的字符串字段",
	"Name of the field to set the parsed JSON data to": "将解析的JSON数据设置为的字段的名称",
	"Specify the topic to deliver the message to, for example \"finance/stock/cmp\"": "指定传递消息的主题，例如“finance / stock / cmp”",
	"Topic": "话题",
	"Whether or not the publish message should be retained by the messaging engine": "发布消息是否应该由消息传递引擎保留",
	"Retain the Message": "保留消息",
	"The field containing XML to flatten.": "包含XML的字段变平。",
	"Field to Flatten": "领域变平",
	"Whether all fields in original record should be kept. If this is set, the root field of the record must be a Map or List Map.": "是否应保留原始记录中的所有字段。如果设置了该值，则记录的根字段必须是地图或列表地图。",
	"Keep Original Fields": "保留原始字段",
	"Overwrite Existing Fields": "覆盖现有字段",
	"Output field into which the XML will be flattened. Use empty value to write directly to root of the record.": "输出字段，XML将被放平。使用空值直接写入记录的根目录。",
	"XML element used to delimit records. If this is not specified, only a single record is generated.": "XML元素用于分隔记录。如果未指定，则只生成一条记录。",
	"Record Delimiter": "记录分隔符",
	"The string used to separate entity names in the flattened field names.": "用于分隔拼合字段名称中的实体名称的字符串。",
	"Field Delimiter": "字段分隔符",
	"The string used to separate attributes in the flattened field names.": "用于分隔拼合字段名称中的属性的字符串。",
	"Attribute Delimiter": "属性分隔符",
	"Whether attributes of elements should be ignored.": "元素的属性是否应该被忽略。",
	"Ignore Attributes": "忽略属性",
	"Whether namespace URIs should be ignored.": "是否应该忽略名称空间URI。",
	"Ignore Namespace URI": "忽略名称空间URI",
	"Records that match the condition pass to the stream": "符合条件的记录传递给流",
	"Condition": "条件",
	"Values": "值",
	"Whether to perform a bulk lookup of all keys in the batch, or perform individual lookups per key.": "是否对批处理中的所有键执行批量查找，或者对每个键执行单独的查找。",
	"Lookup Parameters": "查找参数",
	"File name suffix e.g.'txt'": "文件名后缀eg'txt'",
	"Files Suffix": "文件后缀",
	"The directory is defined by the 'targetDirectory' record header attribute instead of the Directory Template configuration property.": "该目录由“targetDirectory”记录标题属性定义，而不是“目录模板”配置属性。",
	"Directory in Header": "目录中的标题",
	"Directory Template": "目录模板",
	"Time zone to use to resolve directory paths": "用于解析目录路径的时区",
	"Data Time Zone": "数据时区",
	"Time Basis": "时间基础",
	"Number of records that triggers the creation of a new file. Use 0 to opt out.": "触发创建新文件的记录数。使用0退出。",
	"Max Records in File": "文件中的最大记录数",
	"Exceeding this size triggers the creation of a new file. Use 0 to opt out.": "超过此大小会触发创建新文件。使用0退出。",
	"Maximum time for a file to remain idle. After no records are written to a file for the specified time, the destination closes the file. Enter a number to specify a value in seconds. You can also use the MINUTES or HOURS constants in an expression. Use -1 to opt out of a timeout.": "文件保持空闲的最长时间。在指定时间内没有任何记录写入文件后，目标将关闭文件。输入一个数字以秒为单位指定一个值。您也可以在表达式中使用MINUTES或HOURS常量。使用-1选择退出超时。",
	"Idle Timeout": "空闲超时",
	"Compression Codec": "压缩编解码器",
	"Use the full class name": "使用完整的课程名称",
	"Compression Codec Class": "压缩编解码器类",
	"File Type": "文件类型",
	"Sequence File Key": "序列文件密钥",
	"Late Record Time Limit (secs)": "延迟记录时间限制（秒）",
	"Closes the current file and creates a new file when processing a record with the specified roll attribute": "关闭当前文件并在处理具有指定卷属性的记录时创建新文件",
	"Use Roll Attribute": "使用卷属性",
	"Name of the roll attribute": "卷属性的名称",
	"Roll Attribute Name": "滚动属性名称",
	"Action for records considered late.": "对延迟记录采取行动。",
	"Late Record Handling": "晚记录处理",
	"Late Record Directory Template": "晚记录目录模板",
	"When checked, HDFS destination will create test file in configured target directory to verify access privileges.": "勾选后，HDFS目标将在已配置的目标目录中创建测试文件以验证访问权限。",
	"Validate HDFS Permissions": "验证HDFS权限",
	"Expression that determines the target file permissions.Should be a octal/symbolic representation of the permissions.": "决定目标文件权限的表达式。应该是权限的八进制/符号表示。",
	"Permissions Expression": "权限表达",
	"Set to true to skip finding old temporary files that were written to and automatically recover them.": "设置为true以跳过查找写入并自动恢复它们的旧临时文件。",
	"Skip file recovery": "跳过文件恢复",
	"Data Format of the response.": "响应的数据格式。",
	"String field that contains the data to parse": "包含要解析的数据的字符串字段",
	"Name of the field to set the parsed data to": "设置解析数据的字段的名称",
	"Zips the specified fields into a single list.": "将指定的字段压缩到单个列表中。",
	"Fields to Zip": "字段到邮政编码",
	"If checked, the output will be a list of two-element lists. If unchecked, the output will be a list of maps with each field value identified by the originating field name.": "如果选中，输出将是一个两元素列表。如果未选中，则输出将是一个映射列表，其中每个字段值由始发字段名称标识。",
	"Zip Values Only": "仅邮政价值",
	"If 'Record by Record' the processor takes care of record error handling, if 'Batch by Batch' the JavaScript must take care of record error handling": "如果'按记录记录'处理器负责记录错误处理，如果'按批处理'JavaScript必须处理记录错误处理",
	"Record Processing Mode": "记录处理模式",
	"Place initialization code here. Called on pipeline validate/start.": "在此放置初始化代码。调用流水线验证/启动。",
	"Init Script": "初始脚本",
	"Script": "脚本",
	"Place cleanup code here. Called on pipeline stop.": "在这里放置清理代码。调用流水线停止。",
	"Destroy Script": "销毁脚本",
	"String field that contains a LOG line": "包含LOG行的字符串字段",
	"Name of the new field to set the parsed JSON data": "新字段的名称，用于设置已解析的JSON数据",
	"New Parsed Field": "新的解析字段",
	"The regular expression which is used to parse the log line": "用于解析日志行的正则表达式",
	"The grok pattern which is used to parse the log line.": "用于解析日志行的grok模式。",
	"Custom Format": "自定义格式",
	"Type of schema that should be generated.": "应该生成的模式的类型。",
	"Schema Type": "架构类型",
	"Name of the schema that will be generated.": "将生成的模式的名称。",
	"Schema name": "架构名称",
	"Name of the header attribute where the generated schema will be stored.": "将生成的模式存储在其中的标题属性的名称。",
	"Namespace for generated schema.": "生成模式的命名空间。",
	"Namespace": "命名空间",
	"Documentation string that will be stored in the generated schema.": "将存储在生成的模式中的文档字符串。",
	"Doc": "文件",
	"If set to true all schema fields will be created as unions allowing null values.": "如果设置为true，则所有架构字段将被创建为允许空值的联合。",
	"Nullable fields": "可空字段",
	"Set default values to Null.": "将默认值设置为Null。",
	"Default to Nullable": "默认为可空",
	"For SDC types that Avro does not have direct equivalent, expand them to the closest 'bigger' equivalent.": "对于Avro没有直接等价的SDC类型，将它们扩展到最接近的“更大”等价物。",
	"Expand Types": "展开类型",
	"Enables to configure different default value for each type.": "可以为每种类型配置不同的默认值。",
	"Default Values for Types": "类型的默认值",
	"Name of the field attribute that stores precision for decimal fields.": "存储精度为小数字段的字段属性的名称。",
	"Precision Field Attribute": "精度字段属性",
	"Name of the field attribute that stores scale for decimal fields.": "存储小数字段比例的字段属性的名称。",
	"Scale Field Attribute": "比例字段属性",
	"Default precision when given field attribute does not exists or is invalid. Use -1 to disable default value.": "给定字段属性时的默认精度不存在或无效。使用-1来禁用默认值。",
	"Default Precision": "默认精度",
	"Default scale when given field attribute does not exists or is invalid. Use -1 to disable default value.": "给定字段属性时的默认缩放比例不存在或无效。使用-1来禁用默认值。",
	"Default Scale": "默认比例",
	"Rather then calculating schema for each individual record, cache schema and re-use it for the logicallysame records.": "而是计算每个单独记录的模式，缓存模式并将其重新用于逻辑同名记录。",
	"Enable Cache": "启用缓存",
	"Number of schemas that will be held at memory at one time.": "一次将在内存中保存的模式数量。",
	"Cache Size": "缓存大小",
	"Expression used to create cache key. Generator assumes that records with the same evaluated key will always share the same schema.": "用于创建缓存键的表达式。 Generator假定具有相同评估键的记录将始终共享相同的模式。",
	"Cache Key Expression": "缓存键表达",
	"Variables that will be provided to the script as environment variables.": "将作为环境变量提供给脚本的变量。",
	"Environment variables": "环境变量",
	"How long will the script be allowed to run. The time is in milliseconds.": "脚本将允许运行多长时间。时间以毫秒为单位。",
	"Timeout (ms)": "超时（ms）",
	"Maximum number of datagram packets that will be kept in the intermediate queue.": "将保留在中间队列中的数据报数据包的最大数量。",
	"Packet Queue Size": "数据包队列大小",
	"Number of worker threads processing packets from the intermediate queue and running the pipeline.": "处理来自中间队列并运行管道的数据包的工作线程数。",
	"Number of Worker Threads": "工作者线程数",
	"Log Name": "日志名称",
	"Windows log to read from": "Windows日志读取",
	"Read all events in the log or only new events that occur after the pipeline starts": "读取日志中的所有事件或只读取管道启动后发生的新事件",
	"Read Mode": "读取模式",
	"Sensor Device Family": "传感器器件系列",
	"Sensor Device": "传感器设备",
	"I2C Address in hexadecimal string": "I2C地址，十六进制字符串",
	"I2C Address": "I2C地址",
	"Milliseconds to wait before sending the next batch": "发送下一批之前要等待的毫秒数",
	"Silently discard some records": "无声地丢弃一些记录",
	"Discard Some Records": "丢弃一些记录",
	"Fields to generate of the indicated type": "生成指定类型的字段",
	"Fields to Generate": "生成字段",
	"Field Type for root object": "根对象的字段类型",
	"Attributes to be put in the generated record header": "要放入生成的记录标题中的属性",
	"Header Attributes": "标题属性",
	"Number of records that will be generated for single batch.": "将为单个批次生成的记录数。",
	"Batch size": "批量大小",
	"Number of concurrent threads that will be generating data in parallel.": "将并行生成数据的并发线程数。",
	"Name of event that should be used when generating events.": "生成事件时应使用的事件名称。",
	"Event name": "活动名称",
	"Name of the Long fields to generate. Enter a comma separated list.": "要生成的长字段的名称。输入逗号分隔的列表。",
	"Max Records to Generate": "最大记录生成",
	"Raw Data": "原始数据",
	"Stop After First Batch": "第一批后停止",
	"Pipeline whose metrics are to be aggregated": "要汇总度量标准的流水线",
	"Pipeline Configuration": "管道配置",
	"Rules Configuration of pipeline whose metrics are to be aggregated": "规则将被聚合的管道的配置",
	"Pipeline Rules Configuration": "管道规则配置",
	"The URL of the pipeline which must be included in the email alerts": "必须包含在电子邮件警报中的管道的URL",
	"Pipeline Url": "管道网址",
	"The target URL from which the latest aggregated metrics can be fetched": "可从中获取最新聚合度量标准的目标网址",
	"Remote Timeseries URL": "远程时间序列网址",
	"The auth token generated by DPM": "由DPM生成的身份验证令牌",
	"Auth Token": "验证令牌",
	"Sdc Id": "Sdc Id",
	"Job Id": "工作ID",
	"Max no of retries to fetch latest metrics from time-series DPM App. To retry indefinitely, use -1. The wait time between retries starts at 2 seconds and doubles until reaching 16 seconds.": "从时间序列DPM应用程序获取最新指标的最大重试次数。要无限期重试，请使用-1。重试之间的等待时间从2秒开始，双倍增加至16秒。",
	"Number of alert texts to retain in memory": "要保留在内存中的警报文本的数量",
	"Alert Texts To Retain": "警报文本保留",
	"The target URL into which the data must be written": "数据必须写入的目标URL",
	"Target URL": "目标网址",
	"Pipeline commit Id": "管道提交ID",
	"Time to wait between stats updates": "等待状态更新之间的时间",
	"Wait Time (ms) between updates": "等待更新之间的时间（毫秒）",
	"If enabled, compresses the request body using snappy": "如果启用，请使用snappy压缩请求主体",
	"Compress Requests": "压缩请求",
	"Max no of retries to write to time-series DPM App. To retry indefinitely, use -1. The wait time between retries starts at 2 seconds and doubles until reaching 60 seconds.": "写入时间序列DPM应用程序的最大重试次数。要无限期重试，请使用-1。两次重试之间的等待时间从2秒开始，双倍增加至60秒。",
	"Use format mongodb://host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]": "使用格式mongodb：// host1 [：port1] [，host2 [：port2]，... [，hostN [：portN]]] [/ [database] [？options]]",
	"Connection String": "连接字符串",
	"Database": "数据库",
	"Collection": "Collection",
	"For delegated authentication, specify alternate database name. Leave blank for normal authentication": "对于委派身份验证，请指定备用数据库名称。保留空白以进行正常验证",
	"Authentication Source": "身份验证源",
	"Sets the maximum number of connections per host": "设置每台主机的最大连接数",
	"Connections Per Host": "每个主机的连接",
	"Sets the minimum number of connections per host": "设置每个主机的最小连接数",
	"Min Connections Per Host": "每台主机最小连接数",
	"Sets the connection timeout": "设置连接超时",
	"Sets the maximum idle time for a pooled connection": "设置池中连接的最大空闲时间",
	"Max Connection Idle Time": "最大连接空闲时间",
	"Sets the maximum life time for a pooled connection": "设置池式连接的最长使用时间",
	"Max Connection Life Time": "最大连接寿命",
	"Sets the maximum time that a thread will block waiting for a connection": "设置线程阻塞等待连接的最长时间",
	"Max Wait Time": "最长等待时间",
	"Sets the server selection timeout in milliseconds, which defines how long the driver will wait for server selection to succeed before throwing an exception": "设置服务器选择超时（以毫秒为单位），该选项定义驱动程序在抛出异常之前等待服务器选择成功的时间",
	"Server Selection Timeout": "服务器选择超时",
	"Threads Allowed To Block For Connection Multiplier": "允许线程阻塞连接乘数",
	"Sets the heartbeat frequency": "设置心跳频率",
	"Heartbeat Frequency": "心跳频率",
	"Sets the minimum heartbeat frequency": "设置最低心跳频率",
	"Min Heartbeat Frequency": "最小心跳频率",
	"Sets the connect timeout for connections used for the cluster heartbeat": "设置用于群集心跳的连接的连接超时",
	"Heartbeat Connect Timeout": "心跳连接超时",
	"Sets the socket timeout for connections used for the cluster heartbeat": "设置用于群集心跳的连接的套接字超时",
	"Heartbeat Socket Timeout": "心跳插口超时",
	"Sets the local threshold": "设置本地阈值",
	"Local Threshold": "本地阈值",
	"Sets the required replica set name for the cluster": "为集群设置所需的副本集名称",
	"Required Replica Set Name": "所需的副本集名称",
	"Sets whether cursor finalizers are enabled": "设置是否启用游标结束符",
	"Cursor Finalizer Enabled": "光标终结器已启用",
	"Sets whether socket keep alive is enabled": "设置是否启用套接字保持活动状态",
	"Socket Keep Alive": "套接字保持活跃",
	"Sets the socket timeout": "设置套接字超时",
	"Socket Timeout": "套接字超时",
	"Sets whether to use SSL": "设置是否使用SSL",
	"SSL Enabled": "SSL已启用",
	"Define whether invalid host names should be allowed": "定义是否允许使用无效的主机名",
	"SSL Invalid Host Name Allowed": "允许SSL无效的主机名称",
	"Unique key field is required for update and replace while optional for inserts and deletes": "更新和替换需要唯一键字段，插入和删除可选则需要",
	"Unique Key Field": "唯一主键",
	"Sets the Upsert flag for Update and Replace operations": "设置更新和替换操作的Upsert标志",
	"Upsert": "UPSERT",
	"Sets the write concern": "设置写入关注点",
	"Write Concern": "写关注",
	"Specify the initial timestamp in seconds. Leave -1 to opt out.": "指定以秒为单位的初始时间戳。让-1退出。",
	"Initial Timestamp (secs)": "初始时间戳（秒）",
	"Specify the initial ordinal after timestamp. Leave -1 to Opt out.": "指定时间戳后的初始序号。让-1退出。",
	"Initial Ordinal": "初始序数",
	"Oplog Operation types to read": "Oplog要读取的操作类型",
	"Operation Types": "操作类型",
	"Connects to the first MongoDB server in the connection string": "连接到连接字符串中的第一个MongoDB服务器",
	"Enable Single Mode": "启用单一模式",
	"Batch Size (records)": "批量大小（记录）",
	"Max Batch Wait Time": "最大批量等待时间",
	"Sets the read preference": "设置读取首选项",
	"Read Preference": "阅读偏好设置",
	"Un-check this box if querying an uncapped collection.": "如果查询未压缩的collection，请取消选中此框。",
	"Capped Collection": "压缩的collection",
	"Must be provided in timestamp format: YYYY-MM-DD HH:mm:ss if offset field is ObjectId type. If offset field is String type, provide an initial string. Oldest data to be retrieved.": "必须以时间戳格式提供：YYYY-MM-DD HH：mm：ss如果偏移量字段是ObjectId类型。如果偏移字段是字符串类型，请提供一个初始字符串。最旧的数据被检索。",
	"Initial Offset": "初始偏移",
	"Offset field type. Currently ObjectId and String types are supported.": "偏移字段类型。目前支持ObjectId和String类型。",
	"Offset Field Type": "偏移字段类型",
	"Field checked to track current offset.": "检查字段以跟踪当前的偏移量。",
	"Offset Field": "偏移字段",
	"SELECT <column>, ... FROM <table name> WHERE <column> <operator>  <expression>": "SELECT <column>，... FROM <表名> WHERE <column> <运算符> <表达式>",
	"SQL Query": "SQL查询",
	"Mappings from column names to field names": "从列名到字段名的映射",
	"Column Mappings": "列映射",
	"How to handle multiple values": "如何处理多个值",
	"How to handle missing values": "如何处理缺失的值",
	"Missing Values Behavior": "缺少价值行为",
	"Max Clob Size (Characters)": "Max Clob大小（字符）",
	"Max Blob Size (Bytes)": "最大Blob大小（字节）",
	"JDBC Connection String": "JDBC连接字符串",
	"Additional properties to pass to the underlying JDBC driver.": "要传递给底层JDBC驱动程序的其他属性。",
	"Additional JDBC Configuration Properties": "其他JDBC配置属性",
	"Class name for pre-JDBC 4 compliant drivers.": "JDBC-4以前的驱动程序的类名。",
	"JDBC Driver Class Name": "JDBC驱动程序类名称",
	"Not recommended for JDBC 4 compliant drivers. Runs when a new database connection is established.": "不建议用于符合JDBC 4的驱动程序。在建立新的数据库连接时运行。",
	"Connection Health Test Query": "连接健康测试查询",
	"Maximum number of connections to create to the data source": "要创建到数据源的最大连接数",
	"Maximum Pool Size": "最大池大小",
	"Minimum number of connections to maintain. It is recommended to set this to the same valueas Maximum Pool Size which effectively creates a fixed connection pool.": "要维护的最小连接数。建议将其设置为最大池大小相同的值，以有效创建固定连接池。",
	"Minimum Idle Connections": "最小空闲连接",
	"Maximum time to wait for a connection to become available. Exceeding will cause a pipeline error.": "等待连接可用的最长时间。超过会导致管道错误。",
	"Connection Timeout": "连接超时",
	"Maximum amount of time that a connection is allowed to sit idle in the pool. 0 means don't remove idle connections.": "允许连接在池中闲置的最长时间。 0表示不删除空闲连接。",
	"Maximum lifetime of a connection in the pool. When reached it will be retired from the pool. 0 means no maximum lifetime.": "池中连接的最大生命周期。到达后，它将从池中退出。 0意味着没有最长的使用期",
	"Max Connection Lifetime": "最大连接寿命",
	"Whether the connection should have property auto-commit set to true or not.": "连接是否应该将属性自动提交设置为true。",
	"Auto Commit": "自动提交",
	"Should be set to true whenever possible to avoid unintended writes. Set to false with extreme caution.": "应尽可能设置为true以避免意外写入。极其谨慎地设置为false。",
	"Enforce Read-only Connection": "强制只读连接",
	"SQL query that will be executed on all new connections when they are created, before they are added to connection pool.": "SQL查询将在所有新连接创建时在它们被添加到连接池之前执行。",
	"Init Query": "初始化查询",
	"Transaction isolation that should be used for all database connections.": "应该用于所有数据库连接的事务隔离。",
	"Transaction isolation": "事务隔离",
	"Select to enable caching of lookups. This improves performance, but should only be used when values rarely change": "选择启用查找缓存。这可以提高性能，但只能在值很少更改时才使用",
	"Enable Local Caching": "启用本地缓存",
	"Maximum number of values to cache. If exceeded, oldest values are evicted to make room. Default value is -1 which is unlimited": "要缓存的最大值数量。如果超过，最古老的价值被驱逐出房。默认值是-1，这是无限的",
	"Maximum Entries to Cache": "缓存的最大条目",
	"Policy type used to evict values from the local cache. Select whether to reset the expiration time after the last write or after the last access of the value.": "用于从本地缓存中清除值的策略类型。选择是在上一次写入后还是在最后一次访问值后重置过期时间。",
	"Eviction Policy Type": "Eviction策略类型",
	"Expiration Time": "到期时间",
	"Time Unit": "时间单位",
	"By default, the cache 'remembers' that look up for given key failed and always returns default value. This is to avoid doing un-necessary look ups for known missing values. Select this option if new values can be inserted later on and the cache should retry the request rather than returning the cached default value.": "默认情况下，查找给定密钥失败的缓存'记住'并始终返回默认值。这是为了避免对已知缺失值进行不必要的查找。如果稍后可以插入新值，并且缓存应重试请求而不是返回缓存的默认值，请选择此选项。",
	"Retry on Cache Miss": "重试缓存小姐",
	"Number of queries that can be run per second by this source.  Set to zero for no limit.": "此源可以每秒运行的查询数。无限制设置为零。",
	"Queries Per Second": "每秒查询次数",
	"Max Batch Size (Records)": "最大批量（记录）",
	"The number of retries upon a SQL Error.  To allow for the possibility of handling transient errors (ex: connection issues, deadlock, etc.), provide a positive value here.  After the specified number of retries is reached, the stage will fail upon the next error and pipeline error handling behavior will take over.": "SQL错误时重试的次数。为了能够处理瞬时错误（例如：连接问题，死锁等），请在此处提供一个正值。达到指定的重试次数后，阶段将在下一个错误发生时失败，并且管道错误处理行为将接管。",
	"Number of Retries on SQL Error": "SQL错误的重试次数",
	"New Table Discovery Interval": "新表发现间隔",
	"Table Configs": "表配置",
	"Include the latest data in the record": "在记录中包含最新数据",
	"Determines the strategy for each batch to generate records from.": "确定每批生成记录的策略。",
	"Per Batch Strategy": "每批策略",
	"Determines the number of batches that can be generated from the fetched result set after which result set is closed. Leave -1 to keep the result set open as long as possible": "确定结果集关闭后可以从提取结果集中生成的批次数。保留-1以尽可能保持结果集打开",
	"Batches from Result Set": "结果集中的批次",
	"Determines how many open statements/result sets can be cached. Leave -1 to Opt Out and have one statement open per table.": "确定可以缓存多少个打开的语句/结果集。让-1退出，每个表格打开一条语句。",
	"Result Set Cache Size": "结果集缓存大小",
	"Time zone to use to resolve time based expressions": "用于解析基于时间的表达式的时区",
	"Determines the strategy for initial table ordering": "确定初始表格排序的策略",
	"Initial Table Order Strategy": "初始表顺序策略",
	"Fetch Size for the JDBC Statement. Should not be 0": "获取JDBC语句的大小。不应该是0",
	"Fetch Size": "抓取大小",
	"Disabling Incremental Mode will always substitute the value in Initial Offset in place of $ {OFFSET} instead of the most recent value of <offsetColumn>.": "禁用增量模式将始终将“初始偏移”中的值替换为$ {OFFSET}，而不是最新的<offsetColumn>值。",
	"Incremental Mode": "增量模式",
	"SELECT <offset column>, ... FROM <table name> WHERE <offset column>  >  $ {OFFSET} ORDER BY <offset column>": "SELECT <offset列>，... FROM <表名> WHERE <偏移列>> $ {OFFSET} ORDER BY <偏移列>",
	"Initial value to insert for $ {offset}. Subsequent queries will use the result of the Next Offset Query": "为$ {offset}插入的初始值。后续查询将使用Next Offset Query的结果",
	"Column checked to track current offset.": "检查列以跟踪当前偏移量。",
	"Offset Column": "偏移列",
	"Query Interval": "查询间隔",
	"When reading a change data table, column identifying the transaction the change belongs to.": "读取更改数据表时，标识变更所属transaction的列。",
	"Transaction ID Column Name": "Transaction ID列名称",
	"If transactions exceed this size, they will be applied in multiple batches.": "如果Transaction超过这个规模，它们将分批次应用。",
	"Max Transaction Size": "最大Transaction规模",
	"Generates record header attributes that provide additional details about source data, such as the original data type or source table name.": "生成记录标题属性，提供有关源数据的其他详细信息，例如原始数据类型或源表名称。",
	"Create JDBC Header Attributes": "创建JDBC标题属性",
	"Prefix for the header attributes, used as follows: <prefix>.<field name>.<type of information>. For example: jdbc.<field name>.precision and jdbc.<field name>.scale": "标题属性的前缀，用法如下：<前缀>。<字段名称>。<信息类型>。例如：jdbc。<字段名称> .precision和jdbc。<字段名称> .scale",
	"JDBC Header Prefix": "JDBC头部前缀",
	"Disables the validation query and does not validate query formatting such as presence of $ {OFFSET} or ORDER BY clause.": "禁用验证查询并且不验证查询格式，例如存在$ {OFFSET}或ORDER BY子句。",
	"Disable Query Validation": "禁用查询验证",
	"Action that should be performed when an unknown type is detected in the result set.": "在结果集中检测到未知类型时应执行的操作。",
	"On Unknown Type": "未知数据类型",
	"Schema Name": "Schema名称",
	"Table Names should contain only table names. Schema should be defined in the connection string or schema configuration": "表名称只能包含表名。模式应该在连接字符串或模式配置中定义",
	"Table Name": "表名称",
	"Optionally specify additional field mappings when input field name and column name don't match.": "输入字段名称和列名称不匹配时，可以指定其他字段映射。",
	"Field to Column Mapping": "字段到列映射",
	"Mappings from generated columns to field names": "从生成的列到字段名称的映射",
	"Generated Column Mappings": "生成的列映射",
	"Use for lower or mixed-case database, table and field names. Select only when the database or tables were created with quotation marks around the names.": "用于较低或混合大小的数据库，表和字段名称。仅在名称周围使用引号创建数据库或表时才选择。",
	"Enclose Table Name": "附上表格名称",
	"If input is a change data capture log, specify the format.": "如果输入是更改数据捕获日志，请指定格式。",
	"Change Log Format": "更改日志格式",
	"Default operation to perform if sdc.operation.type is not set in record header.": "如果未在记录标题中设置sdc.operation.type，则执行缺省操作。",
	"Default Operation": "默认操作",
	"Action to take when operation type is not supported": "不支持操作类型时采取的操作",
	"Unsupported Operation Handling": "不支持的操作处理",
	"Whether to generate multi-row INSERT statements instead of batches of single-row INSERTs": "是否生成多行INSERT语句而不是成批的单行INSERT",
	"Use Multi-Row Operation": "使用多行操作",
	"The maximum number of prepared statement parameters allowed in each batch insert statement when using multi-row inserts. Set to -1 to disable limit.": "使用多行插入时，每个批处理插入语句中允许的最大准备语句参数数量。设置为-1以禁用限制。",
	"Statement Parameter Limit": "声明参数限制",
	"The maximum number of prepared statement stored in cache. Cache is used only when 'Use Multi-Row Operation' checkbox is unchecked. Use -1 for unlimited number of entries.": "准备好的语句存储在缓存中的最大数目。仅当“使用多行操作”复选框未选中时才使用高速缓存。使用-1表示无限数量的条目。",
	"Max Cache Size Per Batch (Entries)": "每个批次的最大缓存大小（条目）",
	"Whether or not to rollback the entire batch on error. Some JDBC drivers provide informationabout individual failed rows, and can insert partial batches.": "是否在错误时回滚整个批次。一些JDBC驱动程序提供有关单个失败行的信息，并且可以插入部分批处理。",
	"Rollback Batch on Error": "在错误时回滚批处理",
	"SQL Query that should be executed for each incoming record.": "应为每个传入记录执行的SQL查询。",
	"Whether the executor should commit each batch or not.": "执行者是否应该批准每批。",
	"Batch Commit": "批量提交",
	"Allow Late Tables": "允许延迟表格",
	"Enable Schema Changes Event": "启用架构更改事件",
	"Enclose Object Names": "包含对象名称",
	"Select to generate multi-row INSERT and DELETE. Significantly improves performance, but not all databases are supporting the syntax.": "选择以生成多行INSERT和DELETE。显着提高性能，但并非所有数据库都支持该语法。",
	"Maximum number of records in a batch": "批次中的最大记录数",
	"Tables to track": "要跟踪的表格",
	"Tables": "表",
	"Operations to capture as records. All other operations are ignored.": "作为记录捕获的操作。所有其他操作都将被忽略。",
	"Operations": "操作",
	"Use for lower or mixed-case database, table and field names. By default, names are changed to all caps. Select only when the database or tables were created with quotation marks around the names.": "用于较低或混合大小的数据库，表和字段名称。默认情况下，名称更改为全部大写。仅在名称周围使用引号创建数据库或表时才选择。",
	"Case Sensitive Names": "区分大小写的名称",
	"The pluggable database containing the database. Required for Oracle 12c if PDB is used.": "包含数据库的可插入数据库。如果使用PDB，则需要Oracle 12c。",
	"PDB": "PDB",
	"Determines where to start reading": "确定从何处开始阅读",
	"Initial Change": "初始更改",
	"Datetime to use for the initial change. Use the following format: DD-MM-YYYY HH24:MM:SS.": "日期时间用于初始更改。使用以下格式：DD-MM-YYYY HH24：MM：SS。",
	"Start Date": "开始日期",
	"System change number to use for the initial change": "系统更改号码用于初始更改",
	"Start SCN": "启动SCN",
	"Location of the LogMiner dictionary": "LogMiner字典的位置",
	"Dictionary Source": "字典来源",
	"Buffer changes in SDC memory or on Disk. Use this to reduce PGA memory usage on the DB": "SDC内存或磁盘上的缓冲区更改。使用它可以减少DB上的PGA内存使用量",
	"Buffer Changes Locally": "本地缓冲区更改",
	"Buffer Location": "缓冲区位置",
	"If uncommitted transactions have gone past the transaction window, discard them. If unchecked, such transactions are sent to error": "如果未提交的Transaction已经超过Transaction窗口，则丢弃它们。如果未选中，则此类Transaction将发送至错误",
	"Discard Old Uncommitted Transactions": "丢弃旧的未提交的事务",
	"Action to take if an unsupported field type is encountered. When buffering locally, the action is triggered immediately when the record is read without waiting for the commit": "在遇到不支持的字段类型时采取的操作。当本地缓冲时，当不等待提交就读取记录时立即触发动作",
	"Unsupported Field Type": "不支持的字段类型",
	"Add values of unsupported fields as unparsed strings to records": "将不支持的字段的值作为未分析的字符串添加到记录中",
	"Add unsupported fields to records": "将不支持的字段添加到记录",
	"Includes null values passed from the database from full supplemental logging rather than not returning those fields.": "包括从完整补充日志记录传递到数据库的空值，而不是返回这些字段。",
	"Include Nulls": "包括空值",
	"Time window to look for changes within a transaction before commit (in seconds)": "提交前查找事务内更改的时间窗口（以秒为单位）",
	"Maximum Transaction Length": "最大Transaction时长",
	"Time window of time a LogMiner session should be kept open. Must be greater than or equal to Maximum Transaction Length. Keeping this small will reduce memory usage on Oracle.": "LogMiner会话的时间窗口应保持打开状态。必须大于或等于最大Transaction时长。保持这个小将会减少Oracle的内存使用量。",
	"LogMiner Session Window": "LogMiner会话窗口",
	"Time to wait before timing out a LogMiner query and returning the batch.": "在超时LogMiner查询并返回批处理之前需要等待的时间。",
	"Query Timeout": "查询超时",
	"To reduce latency, set this lower if the write rate to the tables is low.": "为了减少延迟，如果写入表的速率很低，请将其设置得更低。",
	"JDBC Fetch Size": "JDBC提取大小",
	"Send the actual redo query returned by LogMiner in record headers": "在记录标题中发送由LogMiner返回的实际重做查询",
	"Send Redo Query": "发送重做查询",
	"Time Zone that the DB is operating in": "数据库运行的时区",
	"DB Time Zone": "数据库时区",
	"Determines the quote character to be used on table / schema / column names during query.": "确定查询过程中要在表/模式/列名称上使用的引号字符。",
	"Decimal Scale Attribute": "十进制尺度属性",
	"Decimal Precision Attribute": "十进制精度属性",
	"Notifications": "通知",
	"Cluster": "集群",
	"Statistics": "统计",
	"Field Pivot": "现场枢轴",
	"HTTP": "HTTP",
	"TLS": "TLS",
	"Base64": "Base64编码",
	"Credentials": "鉴权",
	"OAuth 2": "OAuth 2",
	"Proxy": "代理",
	"Logging": "记录",
	"Deduplication": "重复数据删除",
	"Order": "订购",
	"Rename": "改名",
	"Events": "活动",
	"OPC UA": "OPC UA",
	"NodeIds": "NodeIds",
	"Channel Config": "频道配置",
	"Security": "安全",
	"Conversions": "转换",
	"RPC": "RPC",
	"Advanced": "高级",
	"Expressions": "表达式",
	"TCP": "TCP",
	"Syslog": "系统日志",
	"NetFlow 9": "NetFlow 9",
	"Files": "档",
	"Post Processing": "后期处理",
	"The full path of the file": "文件的完整路径",
	"File Path": "文件路径",
	"CoAP": "CoAP协议",
	"Replace": "更换",
	"Parse": "解析",
	"WebSocket": "的WebSocket",
	"UDP": "UDP",
	"syslog": "系统日志",
	"collectd": "collectd",
	"Raw/Separated data": "原始/分离数据",
	"MQTT": "MQTT",
	"Hash Field": "哈希字段",
	"Hash Record": "哈希记录",
	"Merge": "合并",
	"Pagination": "分页",
	"Timeout Handling": "超时处理",
	"Remove/Keep": "删除/保持",
	"Mask": "面具",
	"Geolocation": "地理位置",
	"ADVANCED": "高级",
	"Email": "电子邮件",
	"Split": "分裂",
	"SFTP/FTP": "SFTP / FTP",
	"Error Handling": "错误处理",
	"Conditions": "条件",
	"Lookup": "抬头",
	"Static Store": "静态商店",
	"Output Files": "输出文件",
	"Late Records": "晚记录",
	"Parser": "分析器",
	"Zip": "压缩",
	"JavaScript": "JavaScript的",
	"Schema": "Schema",
	"Schema Name": "Schema 名称",
	"Pattern of the table names to read. Use a SQL like syntax.": "表名称Pattern, 使用SQL Like语法.",
	"Avro": "Avro",
	"Types": "类型",
	"Environment": "环境",
	"Windows": "视窗",
	"Sensor": "传感器",
	"JDBC": "JDBC",
	"Legacy Drivers": "传统驱动程序",
	"Change Tracking": "更改跟踪",
	"Legacy": "Legacy",
	"Change Data Capture": "更改数据捕获",
	"CDC": "CDC",
	"Oracle CDC": "Oracle CDC",
	"Data that should be included as a part of the Webhook request": "应作为Webhook请求的一部分包含的数据",
	"Payload": "有效载荷",
	"The Webhook HTTP resource URL": "Webhook HTTP资源URL",
	"Webhook URL": "Webhook URL",
	"Content Type": "内容类型",
	"Field Path": "场路径",
	"Regular Expression Group": "正则表达式组",
	"Name": "名称",
	"Value": "值",
	"Existing fields to rename. You can use regular expressions to rename a set of fields.": "现有字段进行重命名。您可以使用正则表达式来重命名一组字段。",
	"Source Field Expression": "源字段表达式",
	"New name for the field. You can use regular expressions to rename a set of fields.": "该字段的新名称。您可以使用正则表达式来重命名一组字段。",
	"Target Field Expression": "目标字段表达式",
	"Enable the aggregation configuration": "启用聚合配置",
	"Enabled": "启用",
	"When there are multiple aggregations, the names identify them uniquely": "当有多个聚合时，名称可以唯一标识它们",
	"Enter the title you want to see in the metrics view for this aggregation": "输入您希望在此聚合的度量视图中查看的标题",
	"Aggregation Title": "聚合标题",
	"Select to enable filtering records for aggregation": "选择启用聚合的过滤记录",
	"Filter": "过滤",
	"An expression to filter records for aggregation": "用于过滤记录以进行聚合的表达式",
	"Filter Predicate": "过滤谓词",
	"Aggregation Function": "聚合函数",
	"The resulting value of this expression will be aggregated": "该表达式的结果值将被聚合",
	"Aggregation Expression": "聚合表达",
	"Group By": "通过...分组",
	"Aggregations will be grouped by the resulting values of this expression": "聚合将根据此表达式的结果值进行分组",
	"Group By Expression": "按表达式分组",
	"Field Name": "字段名称",
	"The identifier for a node in the address space of an OPC UA server": "OPC UA服务器地址空间中节点的标识符",
	"Identifier": "识别码",
	"Identifier Type": "标识符类型",
	"Namespace Index": "命名空间索引",
	"You can convert multiple fields to the same type": "您可以将多个字段转换为相同的类型",
	"Fields to Convert": "要转换的字段",
	"Select a compatible data type": "选择一个兼容的数据类型",
	"Convert to Type": "转换为类型",
	"Select to convert input Long to DateTime before converting to a String": "在转换为字符串之前选择将输入Long转换为DateTime",
	"Treat Input Field as Date": "将输入字段视为日期",
	"Affects the interpretation of locale sensitive data, such as using the comma as a decimal separator": "影响区域敏感数据的解释，例如使用逗号作为小数点分隔符",
	"Data Locale": "数据区域设置",
	"Decimal Value Scale": "十进制值标度",
	"Scale": "规模",
	"Rounding strategy during scale conversion": "缩放转换过程中的舍入策略",
	"Rounding Strategy": "舍入策略",
	"Select or enter any valid date or datetime format": "选择或输入任何有效的日期或日期时间格式",
	"Date Format": "日期格式",
	"Other Date Format": "其他日期格式",
	"Zoned DateTime Format": "分区日期时间格式",
	"Other Zoned DateTime Format": "其他分区日期时间格式",
	"CharSet": "字符集",
	"Converts all fields of given type": "转换给定类型的所有字段",
	"Source type": "来源类型",
	"Use an existing field or enter a new field. Using an existing field overwrites the original value.": "使用现有字段或输入新字段。使用现有字段覆盖原始值。",
	"Use the expression language to modify values in a field.": "使用表达式语言来修改字段中的值。",
	"Field Expression": "场表达",
	"Use an existing header attribute or enter a new attribute. Using an existing attribute overwrites the original value.": "使用现有的标题属性或输入新的属性。使用现有属性会覆盖原始值。",
	"Use the expression language to modify or set new attributes in the header.": "使用表达式语言来修改或设置标题中的新属性。",
	"Header Attribute Expression": "标题属性表达",
	"The existing field which will receive the attribute value.": "将接收属性值的现有字段。",
	"Field": "领域",
	"Use an existing field attribute or enter a new attribute. Using an existing attribute overwrites the original value.": "使用现有的字段属性或输入新的属性。使用现有属性会覆盖原始值。",
	"Attribute Name": "属性名称",
	"Use the expression language to modify or set new attributes in the field.": "使用表达式语言来修改或设置字段中的新属性。",
	"Metadata tag": "元数据标签",
	"Tag": "标签",
	"Full path of the file to tail. If using 'Files matching a pattern' as file naming you must use '$ {PATTERN}' token in the file name of the file path.": "文件尾部的完整路径。如果使用'文件匹配模式'作为文件命名，则必须在文件路径的文件名中使用'$ {PATTERN}'标记。",
	"Path": "路径",
	"Naming": "命名",
	"A Java regular expression to match the '$ {PATTERN}' section in the file name": "一个Java正则表达式，用于匹配文件名中的'$ {PATTERN}'部分",
	"Pattern": "模式",
	"First file to process. Leave empty for all.": "第一个文件处理。全部留空。",
	"First File": "第一个文件",
	"Specify fields to null": "指定字段为空",
	"Condition for replacing the fields with null. Leave empty to replace all the configured fields with null": "用null替换字段的条件。留空以用空替换所有配置的字段",
	"You can enter multiple fields to replace with the same value": "您可以输入多个字段来替换为相同的值",
	"Fields to Replace": "要替换的字段",
	"Value to replace nulls": "取代空值的价值",
	"Replacement Value": "替换值",
	"Specify names of fields": "指定字段的名称",
	"Select comparison operator < = >": "选择比较运算符<=>",
	"Operator": "操作者",
	"Literal value to compare against": "字面值与之比较",
	"Comparison Value": "比较价值",
	"Field(s) that should be replaced.": "应该替换的字段。",
	"Whether the field should be set to NULL or not.": "该字段是否应该设置为NULL。",
	"Set to Null": "设置为空",
	"Replacement value for the given field(s).": "给定字段的替换值。",
	"New value": "新的价值",
	"One or more fields to hash": "一个或多个字段进行散列",
	"Fields to Hash": "字段哈希",
	"The field in the incoming record to merge.": "传入记录中的字段进行合并。",
	"From Field": "从字段",
	"The field to merge into.": "要合并到的字段。",
	"To Field": "现场",
	"Status code for which the other settings apply.  Only non-2xx (i.e. not OK) codes are permitted.": "其他设置适用的状态码。只有非2xx（即不是OK）代码是允许的。",
	"HTTP Status Code": "HTTP状态码",
	"Action to take when the configured status code is received from the HTTP server": "从HTTP服务器收到配置的状态码时采取的操作",
	"Action for status": "对状态采取行动",
	"Mask string fields. You can enter multiple fields for the same mask type.": "掩码字符串字段。您可以为相同的遮罩类型输入多个字段。",
	"Fields to Mask": "掩码领域",
	"Mask Type": "掩码类型",
	"Use # to reveal field values. Other characters replace field values.": "使用＃来显示字段值。其他字符替换字段值。",
	"Custom Mask": "自定义面膜",
	"Regular expression that matches the data and groups them.": "正则表达式匹配数据并对它们进行分组。",
	"Comma separated list of group numbers that must be revealed in the data.": "数据中必须显示的逗号分隔的组号列表。",
	"Groups To Show": "要显示的组",
	"An absolute path or a file under SDC resources directory in GeoIP2 format": "GeoIP2格式的SDC资源目录下的绝对路径或文件",
	"GeoIP2 Database File": "GeoIP2数据库文件",
	"The type of GeoIP2 database being used": "正在使用的GeoIP2数据库的类型",
	"GeoIP2 Database Type": "GeoIP2数据库类型",
	"Use an integer or string field with IP address data in the following format: n.n.n.n": "使用IP地址数据的整数或字符串字段，格式如下：n.n.n.n",
	"Input Field Name": "输入字段名称",
	"Examples: /<field name>": "示例：/ <字段名称>",
	"Output Field Name": "输出字段名称",
	"GeoIP2 Field": "GeoIP2领域",
	"Boolean expression.  Triggers sending an email when true.": "布尔表达式。触发器发送电子邮件时为true。",
	"Email Subject": "电子邮件主题",
	"Email Body": "电子邮件正文",
	"dummy": "假",
	"An EL expression defining the key to use for a lookup.": "定义用于查找的键的EL表达式。",
	"Key Expression": "关键表达",
	"First field to zip.": "第一个领域拉链。",
	"First Field": "第一场",
	"Second field to zip.": "第二个字段为zip。",
	"Second Field": "第二场",
	"Field path for the new zipped field. This field will be created by the processor.": "新的压缩字段的字段路径。该字段将由处理器创建。",
	"Path for Zipped Field": "压缩字段的路径",
	"Avro Type": "Avro类型",
	"Field Type": "字段类型",
	"Precision of the generated decimal.": "生成的十进制的精度。",
	"Precision": "精确",
	"Scale of the generated decimal.": "生成的小数的比例。",
	"scale": "规模",
	"The database column name.": "数据库列名称。",
	"Column Name": "列名称",
	"The field in the record to receive the value.": "记录中的字段接收值。",
	"SDC Field": "SDC领域",
	"The default value to be used when the database returns no row. If not set, the record is sent to error in such a case.": "数据库不返回任何行时使用的默认值。如果没有设置，则在这种情况下记录被发送到错误。",
	"The field type. By default, the column type from the database will be used. But if the field type is provided, it will overwrite the column type. Note that if the default value is provided, the field type must also be provided.": "字段类型。默认情况下，将使用数据库中的列类型。但是，如果提供了字段类型，它将覆盖列类型。请注意，如果提供了默认值，则还必须提供字段类型。",
	"Data Type": "数据类型",
	"Pattern of the table names to read. Use a SQL like syntax.": "要读取的表格名称的模式。使用SQL语法。",
	"Table Name Pattern": "表名称模式",
	"Pattern of the table names to exclude from being read. Use a Java regex syntax. Leave empty if no exclusion needed.": "要从中读取的表格名称的模式。使用Java正则表达式语法。如果不需要排除，请留空。",
	"Table Exclusion Pattern": "要排除的Table名称pattern",
	"Use -1 to opt out of this option": "使用-1选择退出此选项",
	"A parameterized value to use in the JDBC insert statement. Must include a ?.": "在JDBC插入语句中使用的参数化值。必须包括一个？",
	"Parameterized Value": "参数化值",
	"If capture instance is not specified when enabling CDC tables, the default value for Capture Instance Name is <schema>_<table>": "如果在启用CDC表时未指定捕获实例，则捕获实例名称的默认值为<schema> _ <table>",
	"Capture Instance Name": "捕获实例名称",
	"Use -1 to process all data or use the last-saved offset": "使用-1来处理所有数据或使用上次保存的偏移量",
	"Exclusion Pattern": "排除模式",
	"Overrides the primary key(s) as the offset column(s).": "将主键（s）覆盖为偏移列（s）。",
	"Override Offset Columns": "覆盖偏移列",
	"Specify offset column(s) to override default offset column(s)": "指定偏移列以覆盖默认偏移列（s）",
	"Offset Columns": "偏移列",
	"Configure Initial Offset for each Offset Column.": "为每个偏移列配置初始偏移量。",
	"Use non-incremental loading for any tables that do not have suitable keys or offset column overrides defined.  Progress within the table will not be tracked.": "对任何没有定义合适的键或偏移列覆盖的表使用非递增加载。表格内的进度将不会被追踪。",
	"Enable Non-Incremental Load": "启用非增量加载",
	"Multithreaded processing of partitions mode. Required (validation error if not possible), Best effort (use if possible, but don't fail validation if not), or disabled (no partitioning).": "多线程处理分区模式。必需（如果不可能，验证错误），尽力而为（尽可能使用，但不验证，否则验证失败）或禁用（不分区）。",
	"Multithreaded Partition Processing Mode": "多线程分区处理模式",
	"Controls the size of partitions.  This value represents the range of values that will be covered by a single partition.": "控制分区的大小。该值表示将由单个分区覆盖的值的范围。",
	"Partition Size": "分区大小",
	"The maximum number of partitions that can be processed at once. Includes active partitions with rows left to read and completed partitions before they are pruned.": "一次可以处理的最大分区数量。包含活动分区，剩余的行用于读取和完成分区，然后进行修剪。",
	"Max Partitions": "最大分区",
	"Additional conditions to apply for the offset column when a query is issued. These conditions will be a logical AND with last offset value filter already applied by default.": "查询发布时应用于偏移列的附加条件。这些条件将是一个逻辑与，并且上一个偏移值过滤器已被默认应用。",
	"Offset Column Conditions": "Offset字段条件"
	},
	"global": {
		"title": "任务代理",
		"login": "登录",
		"Jobs": "任务",
		"tapdata_dashboard": "Tapdata仪表板",
		"create_new_job": "创建新任务",
		"menu": {
			"account": {
				"title": "用户",
				"assignedRoles": "分配的角色",
				"assignedGroups": "分配的组",
				"loggedInUser": "登录用户",
				"logout": "注销",
				"main": "帐户"
			},
			"admin": {
				"configuration": "配置",
				"enableDPM": "启用控制中心",
				"disableDPM": "禁用控制中心",
				"jvmMetrics": "SDC度量标准",
				"logs": "日志",
				"main": "管理",
				"resetLocalStorage": "重置浏览器本地存储",
				"restart": "重新启动",
				"shutdown": "关机",
				"usersAndGroups": "用户和组"
			},
			"clusterManager": "集群管理器",
			"dpm": "StreamSets控制中心",
			"help": {
				"about": "关于",
				"askStreamSets": "询问StreamSets",
				"chatOnSlack": "在Slack上聊天",
				"joinUserGroup": "加入我们的用户组",
				"main": "帮助",
				"helpContents": "帮助内容",
				"localHelp": "本地帮助",
				"hostedHelp": "托管帮助",
				"register": "注册",
				"restFulAPI": "RESTful API",
				"settings": "设置",
				"supportBundle": "支持包"
			},
			"home": "首页",
			"dpmEnabled": "StreamSets控制中心已启用 -",
			"dpmDisconnectedMode": "断开模式。 StreamSets控制中心已启用 -",
			"packageManager": "程序包管理器",
			"toggleNavigation": "切换导航"
		},
		"form": {
			"actions": "操作",
			"add": "添加",
			"addAndRemoveStages": "添加/删除阶段",
			"allStages": "所有阶段",
			"architecture": "架构",
			"autoArrange": "自动排列",
			"batchNumber": "批号",
			"cancel": "取消",
			"changeRule": "更改规则",
			"captured": "已捕获",
			"close": "关闭",
			"condition": "条件",
			"create": "创建",
			"creating": "正在创建...",
			"createdBy": "创建者",
			"createdOn": "创建于",
			"dataCollectorLabels": "此数据收集器的标签",
			"delete": "删除",
			"deleting": "正在删除...",
			"deleteStage": "删除阶段",
			"deleteStream": "删除选定的流",
			"deleteOriginConfirmation": "删除原点确认",
			"description": "说明",
			"done": "完成",
			"duplicate": "重复",
			"duplicating": "复制...",
			"duplicateStage": "重复的阶段",
			"download": "下载",
			"downloading": "正在下载...",
			"edit": "编辑",
			"email": "电子邮件地址",
			"emailPlaceholder": "输入电子邮件地址",
			"enable": "启用",
			"error": "错误",
			"errorCode": "错误代码",
			"errorMessage": "错误消息",
			"errorMessages": "错误消息",
			"errorRecord": "错误",
			"errors": "错误",
			"eventRecord": "事件记录",
			"execute": "执行",
			"executor": "执行程序",
			"export": "导出",
			"exportWithLibraryDefinitions": "为Control Hub导出",
			"fieldName": "字段名称",
			"fieldHeader": "字段标题",
			"fromStage": "来自节点",
			"fromStageLane": "来自节点流",
			"fromStageLanePredicate": "来自节点流条件",
			"generate": "生成",
			"getThisVersion": "Get",
			"gridView": "网格视图",
			"groups": "组",
			"groupId": "组ID",
			"groupIdPlaceholder": "输入组ID",
			"help": "帮助",
			"hideNameColumn": "隐藏任务ID列",
			"hideValue": "隐藏值",
			"histogram": "按批次直方图记录（5分钟衰减）",
			"id": "ID",
			"import": "导入",
			"importing": "正在导入...",
			"install": "安装",
			"installing": "正在安装...",
			"isOwner": "是所有者",
			"retry": "重试",
			"label": "标签",
			"labels": "标签",
			"lastModifiedBy": "上次修改时间",
			"lastModifiedOn": "上次修改时间",
			"lastStatusChange": "时间戳",
			"lastUpdated": "上次更新",
			"library": "库",
			"listView": "列表视图",
			"loading": "加载",
			"maximizePane": "最大化窗格",
			"message": "消息",
			"metadata": "元数据",
			"metrics": "度量标准",
			"minimizePane": "最小化窗格",
			"name": "名称",
			"nextStage": "下一阶段",
			"newRecord": "新的输出记录",
			"no": "否",
			"noRecords": "No Records to view。",
			"numberOfCopies": "份数",
			"os": "操作系统",
			"outputRecord": "输出记录",
			"owner": "所有者",
			"parameters": "参数",
			"pipeline": "任务",
			"pipelineId": "任务ID",
			"previousStage": "前一阶段",
			"processor": "处理器",
			"processors": "处理器",
			"publish": "发布任务",
			"publishToEdge": "将任务发布到Data Collector Edge",
			"record": "记录",
			"recordHeader": "记录标题",
			"records": "记录",
			"redo": "重做",
			"refresh": "刷新",
			"required": "必需",
			"reset": "重置",
			"restart": "重新启动Data Collector",
			"read": "Read",
			"revertChanges": "还原数据更改",
			"sampleRecords": "示例记录",
			"sampleValue": "示例值",
			"save": "保存",
			"schemaVersion": "架构版本",
			"settings": "设置",
			"share": "分享",
			"showMore": "显示更多...",
			"showValue": "显示值",
			"showNameColumn": "显示任务ID列",
			"source": "起源",
			"stage": "节点",
			"stages": "阶段",
			"stageErrors": "节点错误",
			"state": "状态",
			"status": "状态",
			"step": "使用更改运行",
			"stream": "Stream",
			"subject": "主题",
			"tableView": "表视图",
			"target": "目标",
			"targets": "目的地",
			"timestamp": "时间戳",
			"title": "标题",
			"toStage": "到节点",
			"type": "Type",
			"quickTips": "快速提示",
			"userId": "用户标识",
			"userIdPlaceholder": "输入用户ID",
			"userName": "显示名称",
			"userNamePlaceholder": "输入显示名称",
			"undo": "撤消",
			"uninstall": "卸载",
			"uninstalling": "正在卸载...",
			"update": "更新",
			"upload": "上传",
			"uploading": "正在上传...",
			"user": "用户",
			"users": "用户",
			"username": "登录",
			"value": "值",
			"view": "View",
			"viewLogs": "查看日志",
			"viewConfig": "日志/配置 视图切换",
			"write": "写入",
			"yes": "是的"
		},
		"messages": {
			"info": {
				"deleteOriginConfirmationMsg": "删除原点级重置当前的原点偏移。你确定你要删除？",
				"graphErrorBadgeLabel": "总错误记录+阶段错误。",
				"isDPMPipelineDirty": "任务自上次发布以来已更新",
				"no​​ParametersDefinedMsg": "此任务未定义参数。要在运行时将参数值传递给任务，请首先在任务的“参数”选项卡中定义参数。有关详情，请参阅<a href=\"https://streamsets.com/documentation/datacollector/latest/help/index.html#datacollector/UserGuide/Job_Configuration/JobConfiguration_title.html#concept_rjh_ntz_qr\" target=\"_blank\">使用运行时参数。</A>",
				"no​​PipelineExists": "找不到名为“{{name}}”的任务",
				"no​​DataAvailable": "无可用数据。",
				"originExists": "Origin已经存在。",
				"saveOperationInProgress": "所有更改已保存",
				"savingConfiguration": "正在保存...",
				"unloadMessage": "如果您离开此页面，您将丢失所有未保存的更改，您确定要离开吗？"
			},
			"error": {
				"notAuthorized": "访问被拒绝 - 您无权访问此页面。"
			},
			"validate": {
				"validationFailed": "验证失败",
				"validationErrors": "验证错误",
				"pipelineValidationInProgress": "验证任务...",
				"pipelineValidationSuccess": "验证成功"
			}
		}
	},
	"home": {
		"ago": "前",
		"createDPMUsers": {
			"title": "创建控制中心组和用户"
		},
		"enableDPM": {
			"alreadyEnabledMsg": "控制中心已启用",
			"contactMessage": "请联系<a href=\"mailto:enteprise@streamsets.com\" target=\"_top\"> enteprise@streamsets.com </a>了解StreamSets Enterprise帐户",
			"dpmBaseURL": "控制中心基本URL",
			"dpmBaseURLPlaceholder": "输入控制中心基本URL",
			"dpmUserName": "控制中心用户标识",
			"dpmUserNamePlaceholder": "输入控制中心用户标识",
			"dpmUserPassword": "控制中心用户密码",
			"dpmUserPasswordPlaceholder": "输入控制中心用户密码",
			"headerLabel": "启用StreamSets控制中心",
			"installStatistics": "安装统计信息库",
			"isEnableInProgress": "启用控制中心...",
			"isManagedByClouderaManager": "对于使用Cloudera Manager的安装，您必须使用Cloudera Manager来使Data Collector能够与Control Hub协同任务。 <a href=\"https://streamsets.com/documentation/datacollector/latest/help/#datacollector/UserGuide/DPM/RegisterSDCwithDPM.html\" target=\"_blank\">了解更多... </a>",
			"successMessage": "成功生成认证令牌并更新文件'etc / application-token.txt'和'etc / dpm.properties'。"
		},
		"createPipeline": "创建新任务",
		"commitPipelineHistory": {
			"committer": "承诺者",
			"commitTime": "已提交",
			"headerLabel": "任务提交历史",
			"no提交": "否提交查看。"
		},
		"connectionLost": {
			"headerLabel": "连接丢失",
			"refreshBrowser": "刷新浏览器",
			"retryNow": "立即重试",
			"retrying": "正在重试...",
			"retryingInSeconds": "在{{retryCountDown}}秒内重试..."
		},
		"disableDPM": {
			"headerLabel": "禁用控制集线器",
			"isDisableInProgress": "禁用控制中心...",
			"isManagedByClouderaManager": "对于使用Cloudera Manager的安装，必须使用Control Hub控制台来禁用控制中心。",
			"confirmationMessage": "您确定要禁用控制中心吗？"
		},
		"downloadRemotePipeline": "下​​载已发布的任务",
		"downloadRemote": {
			"actions": "操作",
			"commitMessage": "提交消息",
			"headerLabel": "下载已发布的任务",
			"noPipelines": "No Jobs to view。",
			"pipelineName": "任务名称",
			"pipelineId": "任务ID",
			"version": "版本"
		},
		"dpmInfo": {
			"enableDPM": "启用控制中心",
			"headerLabel": "StreamSets控制中心",
			"learnMore": "了解更多信息"
		},
		"remotePipeline": "已发布的任务",
		"getStarted": "开始",
		"importPipeline": "导入任务",
		"importPipelinesFromArchive": "从存档导入任务",
		"invalidStatus": "任务无效",
		"header": {
			"addLabels": "添加标签",
			"addLabelsTitle": "将标签添加到任务",
			"addLabelsConfirmationMessage": "输入要添加到所选任务的标签。重复的标签将被忽略。",
			"addingLabelsToPipelines": "向任务添加标签...",
			"addStage": "添加节点",
			"alerts": "警报",
			"continueMonitoring": "继续监控",
			"closePreview": "关闭预览",
			"closeSnapshot": "关闭快照",
			"comfortable": "舒适",
			"commitHistory": "提交历史",
			"compact": "紧凑",
			"connectErrorPipeline": "无法连接",
			"connectingPipeline": "正在连接...",
			"cozy": "舒适",
			"createNewPipeline": "创建新任务",
			"disconnectingPipeline": "断开连接...",
			"downloadExecutable": "下载Edge Executable",
			"emptyAlerts": "这很好！您没有任何新的提醒。",
			"finishingPipeline": "完成任务...",
			"forceStop": "强制停止",
			"forceStopConfirmationTitle": "强制停止任务确认",
			"forceStopConfirmationMessage": "您确定要强制停止Job'{{name}}'吗？",
			"forceStopPipelinesConfirmationMessage": "您确定要强制停止在Jobs之下吗？",
			"hideDetails": "隐藏详细信息",
			"pullLatest": "拉最新",
			"importPipeline": "导入任务",
			"importPipelinesFromArchive": "从存档导入任务",
			"issues": "问题",
			"more": "更多",
			"nextBatch": "预览下一批",
			"notifications": "通知",
			"noConstantsDefinedMsg": "没有定义任务常量",
			"pauseMonitoring": "暂停监控",
			"pipeline": "Job",
			"pipelineIssues": "任务问题",
			"preview": "预览",
			"previewConfig": "预览配置",
			"processors": "处理器",
			"refreshPreview": "刷新预览",
			"refreshSnapshot": "刷新快照",
			"remoteOptions": "控制中心选项",
			"retryPipeline": "重试开始任务",
			"revertChanges": "恢复更改",
			"selectorProcessor": "选择器",
			"settings": "设置",
			"showDetails": "显示详细信息",
			"snapshot": "快照",
			"snapshots": "快照",
			"sources": "起源",
			"stageLibrary": "节点图书馆",
			"start": "开始",
			"startingPipeline": "开始任务...",
			"startWithParametersTitle": "从参数开始",
			"stop": "停止",
			"stopConfirmationTitle": "停止任务确认",
			"stopConfirmationMessage": "您确定要停止Job'{{name}}'吗？",
			"stopPipelinesConfirmationMessage": "您确定要停止在Jobs下面吗？",
			"stoppingPipeline": "停止任务...",
			"stoppingPipelines": "停止任务...",
			"targets": "目的地",
			"timezone": "时区",
			"toggleLibraryPane": "切换库窗格",
			"uptime": "正常运行时间",
			"validate": "验证"
		},
		"getStartedWithTutorials": "尝试教程",
		"graphPane": {
			"errorStackTrace": "错误堆栈跟踪",
			"dontShowAgain": "不再显示。",
			"originMissing": "Origin missing",
			"selectExecutor": "选择执行程序以连接...",
			"selectProcessor": "选择要连接的处理器...",
			"selectProcessorToAddInBetween": "选择Processor以在所选流之间添加...",
			"selectTarget": "选择要连接的目标...",
			"startErrorMessage": "任务定义“{{name}}”正在运行。",
			"viewStackTrace": "查看堆栈跟踪"
		},
		"detailPane": {
			"alerts": "警报",
			"emailIDs": "电子邮件ID",
			"errors": "错误",
			"badRecords": "错误记录",
			"badRecordsTab": {
				"mostRecentBadRecords": "最新的错误记录",
				"mostRecentErrorMessages": "最近的错误消息",
				"badRecordsCounts": "错误记录（自上次启动以来）",
				"badRecordsHistogram": "错误直方图记录（5分钟衰减）",
				"errorsCount": "节点错误数（自上次启动以来）",
				"errorsHistogram": "阶段错误直方图（5分钟衰减）"
			},
			"committedOffsets": "提交的偏移量",
			"configuration": "配置",
			"configurationTab": {
				"changeStageLibrary": "更改节点库",
				"linkInformation": "流信息",
				"enterKey": "输入名称",
				"enterPredicate": "输入条件",
				"enterValue": "输入值",
				"general": "概览",
				"info": "信息",
				"loading": "正在加载...",
				"no记录": "无预览记录。",
				"noFieldFound": "找不到字段",
				"pipelineInformation": "任务信息",
				"producingEventsConfig": "生成事件",
				"rawSourcePreview": "原始预览",
				"requiredFieldsSelectionButton": "使用预览数据选择字段",
				"selectFields": "选择字段...",
				"stageInformation": "节点信息",
				"stageDescription": "阶段描述",
				"stageName": "节点名称",
				"stageInstanceName": "节点实例名称",
				"stageLibrary": "节点图书馆",
				"stageType": "节点类型",
				"stageVersion": "Stage版本",
				"switchToBulkMode": "切换到批量编辑模式",
				"switchToSimpleMode": "切换到简单编辑模式"
			},
			"custom": "自定义",
			"dataDriftRules": "数据漂移规则",
			"dataRules": "数据规则",
			"dataSummaryTab": {
				"allSampledRecords": "全部",
				"allSampledRecordsTooltip": "显示所有采样记录",
				"matchedSampledRecords": "匹配",
				"matchedSampledRecordsTooltip": "仅显示符合条件的采样记录",
				"noRulesMessage": "此流没有活动的数据规则。在检查数据的数据规则选项卡中添加规则。",
				"notMatchedSampledRecords": "未匹配",
				"notMatchedSampledRecordsTooltip": "仅显示与条件不匹配的采样记录"
			},
			"definitionsJSONData": "定义",
			"general": "一般",
			"history": "历史",
			"historyTab": {
				"clear历史": "清除历史",
				"clearHistoryConfirmationMessage": "您确定要清除任务“{{name}}”的历史记录吗？",
				"noStatus": "No Status to view。",
				"summaryModal": {
					"recordCountBarChartTitle": "记录数量"
				},
				"viewSummary": "查看摘要..."
			},
			"info": "信息",
			"inspectingData": "检查数据",
			"last5m": "最后5米",
			"last15m": "最后15米",
			"last1h": "过去1小时",
			"last6h": "过去6小时",
			"last12h": "过去12小时",
			"last24h": "最后24小时",
			"last2d": "最后2d",
			"last7d": "最后7天",
			"last30d": "过去30天",
			"latest": "最新",
			"metricAlertRules": "度量标准规则",
			"monitoring": "监控",
			"monitoringJSONData": "任务监控",
			"notifications": "通知",
			"pipelineConfigJSONData": "任务配置",
			"pipelinePreviewJSONData": "预览数据",
			"pipelineRulesJSONData": "任务规则",
			"pipelineStatusJSONData": "任务状态",
			"rawPreview": "原始预览",
			"recordsPerSecond": "（记录/秒）",
			"restURL": "REST响应",
			"rules": "规则",
			"rulesTab": {
				"action": "行动",
				"alertRules": "警报规则",
				"alertText": "警报文本",
				"alertTextPlaceholder": "触发警报时显示的文本",
				"condition": "条件",
				"conditionPlaceholder": "表达式语言中的规则条件",
				"counter": "Counter",
				"dataDriftRule": "数据漂移规则",
				"dataDriftRules": "数据漂移规则",
				"dataRule": "数据规则",
				"dataRules": "数据规则",
				"deleteRuleTooltip": "删除规则",
				"emailIds": "电子邮件ID",
				"enterEmailIdMsg": "。在任务配置电子邮件ID标签中输入电子邮件ID",
				"enableAlert": "启用警报",
				"enableMeter": "启用仪表",
				"enabled": "活跃",
				"gauge": "量表",
				"group": "集团",
				"groupPlaceholder": "Group",
				"histogram": "直方图",
				"issues": "问题",
				"label": "标签",
				"labelPlaceholder": "规则标签",
				"meter": "米",
				"metricAlertRule": "度量标准规则",
				"metricAlertRules": "度量标准规则",
				"metricElement": "公制元素",
				"metricID": "公制ID",
				"metricRules": "度量标准规则",
				"metricType": "度量标准类型",
				"minVolumePlaceholder": "最小音量",
				"newDataRule": "新的数据规则",
				"noDataDriftRules": "无数据漂移规则查看。",
				"noDataRules": "无数据规则查看。",
				"noMetricAlertRules": "无度量警报规则查看。",
				"samplingPercentage": "抽样百分比",
				"samplingPercentagePlaceholder": "百分比",
				"samplingRecords": "取样记录保留",
				"samplingRecordsPlaceholder": "保留的采样记录数",
				"samplingRules": "抽样规则",
				"sendEmail": "发送电子邮件",
				"sendEmailTooltip": "触发警报时发送电子邮件",
				"stream": "Stream",
				"thresholdValue": "阈值",
				"thresholdType": "阈值类型",
				"timer": "计时器",
				"valuePlaceholder": "价值"
			},
			"statsAggregator": "统计聚合器",
			"startEvent": "启动事件",
			"stopEvent": "停止事件",
			"summary": "总结",
			"summaryTab": {
				"alertsTitle": "提醒",
				"batchThroughput": "批处理吞吐量",
				"batchCount": "数据批次计数",
				"idleBatchCount": "空闲批处理计数",
				"chartsLabel": "图表",
				"chartsPlaceholder": "选择图表",
				"currentSourceOffset": "当前源偏移量",
				"currentBatchAge": "当前批处理时间",
				"currentStage": "当前阶段",
				"currentValue": "当前值",
				"dataObserverRunnable": "Data Observer",
				"metricObserverRunnable": "度量观察者",
				"metricsEventRunnable": "指标事件",
				"productionPipelineRunnable": "生产任务",
				"pipelineStartTime": "任务开始时间",
				"pipelineStopTime": "任务停止时间",
				"recordsProcessed": "处理的记录（自上次启动以来）",
				"recordCountBarChartTitle": "记录计数（自上次启动以来）",
				"recordThroughput": "记录吞吐量",
				"rulesConfigLoaderRunnable": "规则配置加载程序",
				"batchProcessingTimer": "批处理定时器（以秒为单位）",
				"frequency": "Frequency（batch / sec）",
				"duration": "持续时间（秒）",
				"timer": "Timer（Percentiles）",
				"histogram": {
					"inputRecords": "输入",
					"outputRecords": "输出",
					"errorRecords": "错误",
					"errors": "阶段错误"
				},
				"lastBatchInputRecordsCount": "最后一批输入记录计数",
				"lastBatchOutputRecordsCount": "最后一批输出记录计数",
				"lastBatchErrorRecordsCount": "上次批错误记录计数",
				"lastBatchErrorMessagesCount": "最后一批错误消息计数",
				"memoryConsumed": "堆内存使用情况",
				"runtimeParameters": "运行时参数",
				"runtimeStatistics": "运行时统计",
				"settingsTitle": "监控设置",
				"slaveSDCInstances": "工人SDC实例",
				"timeInCurrentStage": "当前阶段的时间",
				"timeOfLastReceivedRecord": "收到最后一个记录",
				"threadLastRun": "线程运行状况报告（上次运行时间）",
				"batchStartTime": "批量开始时间",
				"runnersHistogram": "可用任务跑步者直方图（5分钟衰减）",
				"stageBatchProcessingTimer": "节点批处理定时器（以秒为单位）",
				"stageMemoryConsumed": "阶段堆内存使用情况（以MB为单位）",
				"triggered": "触发",
				"runners": "跑步者",
				"totalRunners": "总跑者",
				"availableRunners": "可用的跑步者"
			},
			"timeSeries": "时间序列",
			"timeSeriesTab": {
				"memoryConsumed": "堆内存使用情况",
				"recordCount": "记录数（自上次启动以来）",
				"recordThroughput": "记录吞吐量"
			},
			"webhook": "Webhook"
		},
		"previewPane": {
			"batchSize": "预览批量大小",
			"cancelPreview": "取消预览",
			"configTitle": "预览配置",
			"configuredSource": "Configured Source",
			"previewSource": "预览来源",
			"previewStage": "预览阶段",
			"previewSingleStage": "预览单一阶段",
			"previewMultipleStages": "预览多个阶段",
			"previewRecords": "预览记录",
			"inputData": "输入数据",
			"outputData": "输出数据",
			"noOutputWarning": "未生成输出记录。",
			"noRecords": "No Records to view。",
			"rawPreviewData": {
				"modalTitle": "原始预览数据"
			},
			"rememberMe": "记住配置",
			"runPreview": "运行预览",
			"showFieldType": "显示字段类型",
			"showHeader": "显示记录/字段标题",
			"skipTargets": "跳过目标",
			"snapshotSource": "快照数据",
			"stageErrorsWarning": "{{stageErrorsCount}}阶段错误，请查看错误标签以获取更多详细信息。",
			"stageConfigTitle": "节点配置",
			"streamCondition": "流条件",
			"timeout": "预览超时（以毫秒为单位）",
			"writeToDestinations": "写入目标和执行程序",
			"writeToDestinationsTooltip": "将预览数据写入目标系统（执行程序和目标）",
			"executeLifecycleEvents": "执行任务生命周期事件。",
			"executeLifecycleEventsTooltip": "生成并处理任务生命周期事件，因为任务通常正在运行。",
			"userProvider": "用户提供"
		},
		"library": {
			"commitMessage": "提交消息",
			"commitMessagePlaceholder": "输入提交消息",
			"descriptionPlaceholder": "任务定义说明",
			"deleteConfirmationTitle": "删除确认",
			"deleteConfirmationMessage": "您确定要删除任务配置“{{name}}”吗？",
			"deletePipelinesConfirmationMessage": "您确定要删除以下Jobs吗？",
			"duplicatePipelineDefinition": "复制任务定义",
			"emptyPipelineList": "无需查看。",
			"newPipelineDefinition": "新任务",
			"namePlaceholder": "任务定义标题",
			"nameRequiredValidation": "请输入任务的标题。",
			"nameValidation": "输入字母数字字符，空格和下划线。",
			"processorPlaceholder": "选择处理器...",
			"revertChangesConfirmationTitle": "恢复更改确认",
			"revertChangesConfirmationMessage": "您确定要恢复对已发布版本“v {{version}}”的任务配置所做的更改吗？",
			"revertingChanges": "恢复更改...",
			"sharingSettings": "共享设置",
			"selectUsersAndGroupsPlaceholder": "选择用户和组...",
			"sourcePlaceholder": "选择Origin ...",
			"targetPlaceholder": "选择目标...",
			"transfer权限": "传输权限",
			"updatePermissionsSuccessMessage": "成功更新所有权限",
      "whereToRun": "你想在哪里运行？",
      "system:allPipelines": "所有任务",
      "system:dpmControlledPipelines": "控制集线器控制的任务",
      "system:publishedPipelines": "已发布的任务",
      "system:allPipelines": "所有任务",
      "system:localPipelines": "本地任务",
      "system:runningPipelines": "运行中任务",
      "system:localPipelines": "本地任务",
      "system:nonRunningPipelines": "已停止任务",
      "system:invalidPipelines": "无效任务",
      "system:errorPipelines": "错误任务",
      "system:sharedWithMePipelines": "与我共享任务"
		},
		"noPipelines": "找不到任务",
		"import": {
			"headerLabel": "导入任务",
			"fileUploadPlaceholder": "没有选择文件",
			"browse": "浏览",
			"overwritePipeline": "覆盖任务",
			"createPipelineAndImport": "创建新任务并导入"
		},
		"publish": {
			"headerLabel": "发布任务",
			"listHeaderLabel": "发布任务",
			"pipelineRepository": "控制中心任务存储库"
		},
		"resetOffset": {
			"title": "重置原点",
			"confirmationTitle": "重置原点确认",
			"confirmationMessage": "Data Collector将请求来自原点的所有可用数据，而不是从最后处理的记录继续。这可能会导致处理已经处理的数据。继续？",
			"confirmationMessageBulk": "Data Collector将请求来自原点的所有可用数据，而不是从最后处理的记录继续。这可能会导致处理已经处理的数据，并且这只会应用于支持重置偏移的原始阶段的任务。你确定并想重新设置以下任务的原点吗？",
			"noSupport": "来源'{{label}}'不支持重置操作",
			"successMessage": "重置原点成功。"
		},
		"runningStatus": "任务正在运行",
		"restart": {
			"confirmationMessage": "您确定要重新启动Data Collector吗？",
			"headerLabel": "重新启动收集器",
			"restarting": "重新启动Data Collector ...",
			"successMessage": "重新启动Data Collector ..."
		},
		"shutdown": {
			"confirmationMessage": "您确定要关闭Data Collector吗？",
			"headerLabel": "关闭收集器",
			"shuttingDown": "关闭Data Collector ...",
			"successMessage": "收集器已停止。"
		},
		"snapshotsPane": {
			"captureSnapshot": "捕捉快照",
			"captureSnapshotInProgress": "捕捉...",
			"labelChangeTitle": "单击更改快照标签",
			"modalTitle": "快照",
			"noSnapshots": "无快照",
			"startAndCaptureSnapshot": "开始和捕捉快照"
		},
		"sortColumn": {
			"NAME": "姓名",
			"CREATED": "创建日期",
			"LAST_MODIFIED": "更新日期",
			"STATUS": "状态"
		},
		"title": "任务",
		"updated": "已更新"
	},
	"jvmMetrics": {
		"gc": {
			"title": "垃圾收集",
			"marksweepcount": "标记扫描计数",
			"marksweeptime": "标记扫描时间",
			"scavengecount": "清除计数",
			"scavengetime": "清除时间"
		},
		"memory": {
			"title": "内存",
			"total": "总内存",
			"heap": "堆内存",
			"nonheap": "非堆内存"
		},
		"title": "数据收集器度量标准",
		"threads": {
			"title": "线程",
			"threadDump": "线程转储",
			"runnable": "可运行",
			"timedwaiting": "定时等待",
			"waiting": "等待",
			"blocked": "已阻止",
			"id": "ID",
			"threadName": "线程名称",
			"state": "状态",
			"stackTrace": "堆栈跟踪",
			"userTime": "用户时间（ns）",
			"cpuTime": "CPU时间（ns）",
			"blockedCount": "阻止计数",
			"waitedCount": "等待计数",
			"viewThreadDump": "查看线程转储..."
		},
		"settingsTitle": "数据收集器度量标准设置"
	},
	"logs": {
		"all": "全部",
		"autoFetchContinue": "开始自动刷新",
		"autoFetchPause": "停止自动刷新",
		"download": "下载",
		"emptyMessage": "加载日志...",
		"loadPreviousLog": "加载以前的日志",
		"loadingLog": "正在载入日志...",
		"logConfig": {
			"headerLabel": "日志配置"
		},
		"noLogsMessage": "找不到记录",
		"refreshLog": "刷新日志",
		"title": "日志"
	},
	"errors": {
		"403": "您无权访问该页面。",
		"title": "错误页面！"
	},
	"metrics": {
		"CURRENT_BATCH_AGE": "当前批处理时间",
		"TIME_IN_CURRENT_STAGE": "当前阶段的时间",
		"TIME_OF_LAST_RECEIVED_RECORD": "上次接收记录的时间",
		"LAST_BATCH_INPUT_RECORDS_COUNT": "最后一批输入记录计数",
		"LAST_BATCH_OUTPUT_RECORDS_COUNT": "上次批输出记录计数",
		"LAST_BATCH_ERROR_RECORDS_COUNT": "最后一批错误记录计数",
		"LAST_BATCH_ERROR_MESSAGES_COUNT": "最后一批错误消息计数",
		"COUNTER_COUNT": "计数",
		"HISTOGRAM_COUNT": "计数",
		"HISTOGRAM_MAX": "最大",
		"HISTOGRAM_MIN": "Min",
		"HISTOGRAM_MEAN": "意思是",
		"HISTOGRAM_MEDIAN": "中位数",
		"HISTOGRAM_P50": "50个百分点",
		"HISTOGRAM_P75": "75百分位数",
		"HISTOGRAM_P95": "95百分位数",
		"HISTOGRAM_P98": "98百分位数",
		"HISTOGRAM_P99": "99百分位数",
		"HISTOGRAM_P999": "99.9百分位数",
		"HISTOGRAM_STD_DEV": "标准偏差",
		"METER_COUNT": "计数",
		"METER_M1_RATE": "1分钟",
		"METER_M5_RATE": "5分钟费率",
		"METER_M15_RATE": "15分钟费率",
		"METER_M30_RATE": "30分钟",
		"METER_H1_RATE": "1小时费率",
		"METER_H6_RATE": "6小时费率",
		"METER_H12_RATE": "12小时费率",
		"METER_H24_RATE": "24小时费率",
		"METER_MEAN_RATE": "平均速率",
		"TIMER_COUNT": "计数",
		"TIMER_MAX": "最大",
		"TIMER_MIN": "Min",
		"TIMER_MEAN": "意思是",
		"TIMER_P50": "50百分位数",
		"TIMER_P75": "75百分位数",
		"TIMER_P95": "95百分位数",
		"TIMER_P98": "98百分位数",
		"TIMER_P99": "99百分位数",
		"TIMER_P999": "99.9百分位数",
		"TIMER_STD_DEV": "标准偏差",
		"TIMER_M1_RATE": "1分钟费率",
		"TIMER_M5_RATE": "5分钟费率",
		"TIMER_M15_RATE": "15分钟费率",
		"TIMER_MEAN_RATE": "平均速率"
	},
	"packageManager": {
		"additionalDrivers": "外部库",
		"customRepoUrl": {
			"title": "配置自定义回购URL",
			"repoUrl": "自定义回购网址",
			"urlValidation": ""
		},
		"deleteExtras": {
			"confirmationMessage": "您确定要删除以下其他驱动程序吗？",
			"title": "删除其他驱动程序"
		},
		"header": {
			"customRepoUrl": "自定义回购网址",
			"install": "安装",
			"uninstall": "卸载"
		},
		"install": {
			"confirmationMessage": "你确定要在节点库下面安装吗？",
			"title": "安装节点库"
		},
		"manifestURL": "清单网址",
		"noStageLibrary": "找不到阶段库",
		"noStageLibrariesExtras": "找不到其他驱动程序",
		"title": "程序包管理器",
		"uninstall": {
			"confirmationMessage": "您确定要卸载以下阶段库吗？",
			"title": "卸载Stage Library"
		},
		"uploadExtras": {
			"library": "节点库",
			"title": "安装外部库"
		}
	},
	"register": {
		"headerLabel": "数据收集器激活密钥",
		"successMessage": "成功上传激活密钥"
	},
	"restapi": {
		"title": "数据收集器RESTful API"
	},
	"sdcConfiguration": {
		"configuration": "配置",
		"infoMessage": "这些配置属性在Data Collector的sdc.properties文件中定义。重新启动Data Collector以使更改生效。",
		"title": "数据收集器配置",
		"value": "值"
	},
	"sdcSettings": {
		"title": "设置",
		"helpBar": "隐藏任务创建帮助栏",
		"lineWrapping": "在属性中换行",
		"restResponse": "隐藏REST响应菜单",
		"runPreviewForFieldPaths": "在后台运行预览以显示可用字段"
	},
	"sdcSupportBundle": {
		"description": "说明",
		"downloadingMessage": "下载支持包...",
		"generator": "生成器",
		"infoMessage": "支持包允许您生成一个存档文件，其中包含解决Data Collector各种问题所需的信息。请选择一个或多个包含在存档中的生成器。然后，您可以下载档案文件进行审阅，或直接将其上传到StreamSets。",
		"title": "支持包",
		"uploadedMessage": "支持包已成功上传。"
	},
	"sdcDirectories": {
		"title": "SDC目录",
		"configDir": "配置目录",
		"dataDir": "数据目录",
		"libsExtraDir": "SDC库额外目录",
		"logDir": "日志目录",
		"resourcesDir": "资源目录",
		"runtimeDir": "运行时目录",
		"staticWebDir": "静态Web目录"
	},
	"usersAndGroups": {
		"title": "用户和组"
	}
}